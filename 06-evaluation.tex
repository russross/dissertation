\chapter{Evaluation}\label{cha:evaluation}

Envoy is designed based on assumptions about a platform that does not yet exist. This limits how the system can be evaluated in two ways. The first is scalability. While common bottlenecks can be avoided and the architecture examined for potential scalability limitations, a system without a large-scale implementation can never be fully tested for issues that only appear at large scales. The best that can be achieved is to identify the most likely sources of problems and extrapolate the results of testing on a smaller scale. The architecture can also be evaluated against assumptions about the workloads it is intended to support, which leads to the second limitation: workloads cannot be accurately forecast. Predicting and simulating client access patterns is a difficult problem even for existing environments \cite{ganger95}, and the problem is amplified for service clusters. As a general purpose platform, service clusters are intended to support a wide range of existing workloads and to enable a flourishing ecosystem of computation services that create entirely new workloads. The design and the evaluation must necessarily rely on assumptions about how the system will be used, and those assumptions limit the applicability of the results.

In this chapter I evaluate the Envoy prototype with three principal goals: to measure the impact of specific design choices, including the basic distributed organisation and cache layout, to assess the scalability of the system, and to evaluate Envoy's ability to support the types of workloads expected in service clusters.

\section{Methodology}

The absence of a realistically sized service cluster limits the scope of system-level testing and the usefulness of many application-level results. The performance side-effects of interacting components and the access patterns of different clients are particularly difficult to predict. The performance characteristics of the storage layer depend on design elements and implementation choices beyond the scope of this dissertation. These factors combined with the poor public availability of many comparable systems limit direct comparisons to previous work.

Building and evaluating a prototype of the Envoy file system is not a futile exercise, however. The artefact may reveal little about real-world usage and behaviour, but measuring it can still do much to justify or condemn the design. This section starts by outlining assumptions about service clusters that influence the design, how the design reflects them, and how measuring the prototype helps to evaluate the decisions made. It concludes with a description of the testing environment and the benchmarks used.

\subsection{Assumptions and goals}

Envoy's success in achieving its stated goals is evaluated partly through argument and comparison to other work as described in previous chapters. Other aspects can be tested directly on a small installation and the results extrapolated to larger configurations. While not comprehensive, this approach is consistent with Envoy's role as one part of a complete service cluster environment. Further research and engineering will be necessary before service clusters can be fully evaluated on their own merits. Some of the assumptions that underpin them can be tested directly, while others must rest on arguments and an appeal to past and future work.

\subsubsection{Independent clients}

Though efficient sharing is important, individual clients accessing unshared data dominates most workloads. Even when data sets overlap, time may divide access from different clients, effectively yielding exclusive access that is transferred from one client to the next over time. Envoy seeks to capture these patterns by subdividing territories to distribute file ownership and control, and by dynamically updating the boundaries over time in response to changing usage. When this is successful, or when access to was exclusive in the first place, Envoy resembles a simple client-server system serving files from an administrative virtual machine to a client virtual machine on the same host.

Since one of Envoy's goals is to create this intra-host configuration whenever possible, the evaluation starts by considering system performance under this circumstance. Maximising raw performance is not the focus of the prototype implementation, but comparing it to systems with a similar topology creates baseline expectations for a production system, and examining absolute performance figures helps confirm the sanity of the overall design.

\secref{sec:independent-clients} evaluates the performance of Envoy as a file server within a single host. Composite performance numbers are compared to other client-server systems, and tests using different configurations help to demonstrate where performance costs are embedded in Envoy. In addition, performance for remote hosts is evaluated to show that the drive to localise traffic benefits performance as well as scalability.

\subsubsection{Shared images}

\secref{sec:shared-images} turns the evaluation to shared data. The emphasis of this section is not on performance---which is dependent largely on runtime topology as evaluated in \secref{sec:independent-clients} and the quality of the implementation---but rather on the behaviour of the system under shared workloads. In effect, \secref{sec:shared-images} evaluates Envoy's ability to localise traffic, and \secref{sec:independent-clients} explores the benefits and costs that result from its success or failure.

Some aspects of Envoy's dynamic behaviour cannot be adequately tested in the limited environment available for this evaluation. The Envoy design prizes simplicity and stability in the layout of territories, both to simplify testing and recovery, and to encourage cache sharing even in the presence of runtime conflicts. The latter goal is achieved by delaying territory transfers until traffic is clearly dominated by a remote participant or until a more slight imbalance has persisted for a longer period of time. Resisting change allows the cache on the owning host to serve all participants at the expense of a network hop from the remote client to the envoy. Rearranging boundaries more eagerly may reduce network hops, but it creates some migration latency and the new host may have to prime its cache before service can resume at full speed.

While the parameters controlling dynamic behaviour can be configured, the ideal balance of stability and layout optimality can only be adequately determined with realistic workloads. Oscillating demand could be best handled by reacting quickly and varying the territory layout in lock-step with traffic changes, or being reluctant to change in order to maximise cache efficacy on one of the hosts may prove better. In the absence of a larger test system and realistic workloads to evaluate specific parameters, this evaluation relies on artificial traffic loads that can be controlled and varied to test the response of the system.

\subsubsection{Scalability}

Envoy's scalability is not evaluated directly. To do so convincingly would require a large cluster with a diverse client base, and direct testing on a reduced scale would prove little. The presumed scalability of the system depends on successfully localising as much traffic as possible. If all requests are satisfied by the same physical machine that hosts the client, then a cluster's capacity to host clients grows linearly with the number of machines added to the system. If most requests are handled locally and neighbouring hosts are consulted mainly to resolve runtime conflicts, then scalability is limited primarily by the degree of runtime sharing. Envoy's goal of scalability is based on a design that attempts to approximate that limit.

The storage layer is another source of traffic and inter-host dependence, but it is not fully specified in this work and cannot be evaluated fairly. A large persistent cache on each host reduces the direct load on the storage layer just as it did for AFS \cite{satyanarayanan85}, but by coupling storage nodes with clients hosts and using switched networking, the overall transaction capacity of the storage layer should scale with the number of hosts anyway, and the number of hosts limits the number of clients. The systems considered in \secref{sec:storage-layer-systems} have already demonstrated the scalability of similar storage architectures; conflict management and the coordination of metadata differentiate each system, but in Envoy that burden is not left to the storage layer.

\subsubsection{Features}

Rapid and inexpensive deployment of services requires support from the file system, and is one of the principal motivations for Envoy. The demands of a software ecosystem are difficult to predict, and Envoy's suitability for this environment is likewise difficult to assess. Features allowing rapid cloning and snapshots of file system images contribute to the flexibility of service clusters and are intended to promote the use of commodity software, but their actual impact in a production system is a matter of speculation. The speed with which images can be cloned and new services launched is measured in \secref{sec:image-operations}.

\subsection{Test environment}

\begin{table}[t]
\begin{center}
\begin{tabular}{l|l}
machine      & \textbf{druid} \\ \hline
processor    & AMD Opteron 240 1.4GHz ($\times$2) \\
memory       & 6 GB \\
disk         & Maxtor Diamond Plus 9 160GB SATA/133 7200 RPM \\
network      & Tigon3 BCM95704A7 10/100/1000BaseT Ethernet \\
& \\
machine      & \textbf{skiing} \\ \hline
processor    & AMD Opteron 250 2.4GHz ($\times$2) \\
memory       & 4 GB \\
disk         & Seagate Cheetah 10K.6 73GB Ultra320 SCSI 10000 RPM \\
network      & Tigon3 BCM95703A30 10/100/1000BaseT Ethernet \\
& \\
machine      & \textbf{moonraider} \\ \hline
processor    & Intel Xeon 2.4GHz with Hyper-threading ($\times$2) \\
memory       & 2 GB \\
disk         & Western Digital Caviar 120GB EIDE ATA/100 7200 RPM \\
network      & Intel 82545EM Gigabit Ethernet Controller
\end{tabular}
\end{center}
\caption[Machines used in the evaluation]{Evaluation machines used throughout this chapter. druid is the root server for most tests, while skiing is the remote client for most comparisons. moonraider is used when more than two nodes are required, and both skiing and moonraider host the storage layer services except where noted.}
\label{tab:hardware}
\end{table}

Three test machines, named \emph{druid}, \emph{skiing}, and \emph{moonraider}, are referenced in this chapter. Each runs SUSE Linux Enterprise Server~10 installed to use Xen. Virtual machines are suffixed with a number, where zero is always the administrative VM that controls hardware directly. Other VMs access them through virtualized device interfaces. The administrative VM is configured with 512~MB of memory, and client VMs are given 256~MB each. The machines themselves are described in \tabref{tab:hardware}. All network connections use gigabit ethernet, and all three machines have more RAM than is allocated to the running virtual machines.

Envoy is accessed from clients using the 9p client driver from version 2.6.18 of the Linux kernel, back-ported to the 2.6.16 kernel included in the Linux distribution. When the test requires it, client VMs are booted from Envoy, but for other tests they are each given a private partition. All services on each machine, including all VMs, Envoy's persistent cache, the storage layer object repositories, and file systems exported by NFS and 9p servers, are run from partitions on a single hard drive on each machine.

druid-0 is the root server for all Envoy tests, and the only server for NFS and 9p tests. Additional Envoy nodes run on skiing-0 and moonraider-0, and Envoy clients on each machine connect to their respective nodes unless otherwise noted. NFS and 9p clients on other machines connect to the server on druid-0 in order to compare network overhead and other effects that arise when using multiple machines. The storage layer in this setup consists of two storage instances hosted on skiing-0 and moonraider-0. Additional tests move the moonraider-0 instance to druid-0, but this is noted in the text when it happens.

\subsubsection{Benchmarks}

Bonnie is a benchmark designed to expose performance bottlenecks \cite{bray}. Bonnie is used in this evaluation to provide some baseline performance numbers and to compare Envoy with other file systems. The Linux driver for 9p was also tested using Bonnie, and that evaluation compares some of the performance characteristics of 9p with NFS \cite{hensbergen}. Some similar comparisons are made here, but the emphasis is on Envoy and its architecture rather than the 9p protocol and its driver.

Bonnie runs in six phases, all involving a single file several times larger than the available memory. It starts by writing the file a character at a time, then it scans the file a block at a time, writing a change back to each block as it goes. The next pass reads the file a block at a time, then it rewrites it (without reading) using blocks, followed by a final pass reading the file a character at a time. Finally, it creates three child processes that seek randomly in the file in parallel.

Bonnie does not exercise metadata operations, and the character operations test aspects of the system that are not relevant here. Bonnie also fails to flush all buffers to disk between tests, leading to possibly skewed results in some cases. Nevertheless, it provides a useful starting point in evaluating Envoy that allows comparisons with other published results.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/linux-sizes}
\caption[Distribution of Linux kernel file sizes]{The distribution of file size in the Linux kernel source tree. 18,703 out of the 20,208 files are 32k or smaller and can fit in a single 9p protocol message.}
\label{fig:linux-sizes}
\end{figure}

To further explore the design of Envoy, a more controlled test is necessary. The Linux 2.6.18 kernel source tree provides a large block of data that can be used for various tests. \figref{fig:linux-sizes} shows the distribution of file sizes in the source tree. The kernel has 1238 directories, each holding an average of 16.32 files (not including subdirectories). The distribution is skewed, with a standard distribution of 31.95 and the largest directory holding 713 files. The median size is 8 files. The largest file is 853k, but 92.6\% of the files can be transmitted within a single 32k message using the 9p protocol.

File sizes are significant particularly for read tests when files must be retrieved from the storage layer. Large files are read in parallel from the two storage servers, but for most files a single request to a randomly chosen storage node is sufficient, so parallel reads do not make a significant impact on the results. The prototype implementation also reads the entire file from the storage layer and writes it to the persistent cache before answering any requests from the client. For these reasons, latency will likely affect Linux kernel read tests more than transfer rates.

For write tests, the file tree is extracted from a compressed \texttt{tar} archive hosted on a local disk partition. For each test, the compressed archive is first loaded into memory to prime the cache. When compressed, the archive is small enough to fit in memory on the clients, and decompression is quick enough to have negligible effect on the results. The uncompressed archive is 229~MB, and the extracted tree occupies about 350~MB of disk space due to internal fragmentation.

Reading all the files back into a new (uncompressed) \texttt{tar} archive provides one read test. The archive is piped to \texttt{/dev/null} to avoid incurring any local storage costs. \texttt{rsync} is also used to read the files, again piping all results to \texttt{/dev/null}. \texttt{rsync} differs from \texttt{tar} in that it reads multiple files concurrently, reducing the overhead of latency from synchronous read requests. Both tests scan all directories and read metadata for each file in addition to reading file contents.

Three different cache settings are relevant for Envoy. The first is a cold cache, which requires retrieving all data and metadata from the storage layer. The second is a warm cache, where data is available from the node-local persistent cache, but not in memory. For cold and warm cache tests, all servers are restarted and all partitions remounted to ensure that in-memory buffers are empty. This also forces Envoy to verify persistent cache data with the storage layer before using it, but this is a quick metadata operation that adds little overhead to the results. Hot cache results pull data from the in-memory cache, but server processes are still restarted between tests and the storage layer must still be consulted to verify data validity.

For \texttt{npfs}, a multi-threaded userspace 9p server used for comparison testing, a cold cache environment corresponds most closely to the warm cache setting for Envoy. \texttt{unfsd}, the userspace NFS server, has client-side caching with three basic configurations, where data is retrieved from the server's disk, the server's memory, or the client's local cache memory. In the final case, the server is not restarted and the client not remounted to avoid flushing the cache. Individual tests are accompanied by details of the cache status for each server.

\section{Independent clients}\label{sec:independent-clients}

Much of the complexity in distributed file system design is related to data sharing, but sharing is less common than individual clients accessing private data. A service derived from a stand-alone machine typically requires a private boot image, and lightweight fork operations in Envoy encourage the cloning of entire images for private use by related and unrelated service instances.

To succeed as a distributed file system on a large scale, Envoy must provide a useful service to isolated clients that obviates the need for additional storage services for most clients. While there is no technical or administrative distinction between private and shared images, Envoy's design and runtime behaviour under these two conditions can be evaluated separately. This section focuses on Envoy's performance and behaviour in the absence of sharing.

\subsection{Performance}

The prototype is not optimised, and absolute performance is not the focus of this evaluation. Nevertheless, it is helpful to establish baseline performance numbers to show that Envoy is viable as a file system. For context, the results are compared to \texttt{npfs}, a multi-threaded userspace 9p server, and \texttt{unfsd}, a single-threaded userspace NFS server. Userspace servers have additional overhead when compared to kernel implementations, but the penalty is assumed to be similar for all of the implementations tested. Standard benchmarks do a poor job of isolating the costs involved in Envoy's data paths, and most of the evaluation in this chapter eschews them in favour of more controlled tests, but this section starts with the Bonnie benchmark to provide some raw performance figures comparable to those from other systems.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/bonnie-druid1-write}
\caption[Bonnie benchmark results for block writes]{Bonnie benchmark results for block writes on a 2~GB dataset. Results for Envoy, Envoy using a local storage server instance, Envoy with no persistent cache, a userspace 9p server, and a userspace NFS server are compared. Error bars show the standard deviation over ten runs.}
\label{fig:bonnie-druid1-write}
\end{figure}

\figref{fig:bonnie-druid1-write} shows the block write results of the Bonnie benchmark on a 2~GB dataset. Envoy is competitive with the 9p and NFS servers, despite writing to two storage servers and the local persistent cache. Bonnie does not force a sync after writing, so this represents a conservative result for Envoy. When the write is complete, Envoy has flushed the data to two storage servers and can tolerate a node failure without loss, while the 9p and NFS servers both have a dirty cache on a single node with no replicas.

For sustained writes, the disk is the primary bottleneck, as demonstrated by the result labelled \emph{envoy-ls}, which has one of the two storage servers on the same machine as the envoy. In this case, data must be written twice using the same disk, once for the cache and a second time for the storage instance. In addition, the in-memory cache is split between the two functions, cutting its effective size in half. The \emph{nocache} figure is an Envoy server with the persistent cache disabled, a change that has little effect in this test because the writes are performed in parallel on each node.

Envoy nodes all host storage servers, and it is reasonable to assume that each node generates a similar amount of traffic on average. Storage is distributed throughout the cluster, so it is less likely that a node with a write-heavy client will also host storage for other write-heavy clients at the same time; envoy-ls represents a worst-case scenario rather than a common case.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/bonnie-druid1-read}
\caption[Bonnie benchmark results for block reads]{Bonnie benchmark results for block reads on a 2~GB dataset. Results for Envoy, Envoy using a local storage server instance, Envoy with no persistent cache, a userspace 9p server, and a userspace NFS server are compared. Error bars show the standard deviation over ten runs.}
\label{fig:bonnie-druid1-read}
\end{figure}

\figref{fig:bonnie-druid1-read} shows the block read results from the same benchmark. Bonnie's test uses a single file, so the Envoy servers are reading data from a single file in the persistent cache. All of the tests except nocache involve transfers from a single file in one VM to a client in another VM, with similar overall results. As in the write tests, the results are dominated by disk speeds.

With no persistent cache, Envoy must fetch each block from one of the storage servers. The prototype limits files to one outstanding transaction each, so parallelism is not exploited in this test, and requests are randomly distributed to the two storage servers, so neither ends up with a sequential read through the entire file. A more sophisticated storage layer would likely reduce the penalty of omitting the persistent cache, but in normal operation most data is expected to be in the persistent cache anyway.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/bonnie-druid1-rewrite}
\caption[Bonnie benchmark results for block re-writes]{Bonnie benchmark results for block re-writes on a 2~GB dataset. Each block in the dataset is read, modified, and written back in sequence. Results for Envoy, Envoy using a local storage server instance, Envoy with no persistent cache, a userspace 9p server, and a userspace NFS server are compared. Error bars show the standard deviation over ten runs.}
\label{fig:bonnie-druid1-rewrite}
\end{figure}

Bonnie's re-write test reads the same file in sequence, writing back a change to each block as it is encountered, with results displayed in \figref{fig:bonnie-druid1-rewrite}. The Envoy results reflect the write penalty for a local storage server and the read penalty for no persistent cache, with no surprises. Envoy lags behind 9p in this test, probably because the read is done from the persistent cache, but the write requires each storage server to seek the correct position in the file for each write afterward. Unlike the write test, where the three disks work in parallel, this test forces a seek and read from one disk, followed by a seek and write to all three. For the second seek, the 9p server and the persistent cache are more likely to find the disk head in the right place than the storage servers, which omitted the first seek.

The poor showing from NFS relative to 9p is due to an artifact in the test that interacts with the client-side cache. The rewrite test is preceded by the write test with no explicit \texttt{sync} operation to flush the write cache to disk. All of the re-write tests start with a dirty cache, but NFS adds the client's cache to that of the server, leaving more writes from the previous test that are billed to the current one.

The results from testing with Bonnie show Envoy to be competent at serving files in a simple client-server configuration between two virtual machines. Results are comparable to a simple 9p or NFS server for operations on datasets larger than the in-memory cache size. In these tests, the speed of the disk appears to be the primary bottleneck, which is the expected result for basic operations involving bulk data transfer with few metadata operations. The remainder of this section explores the results in more detail to reveal the effect that the network and virtual machine architecture have on performance and how caching, both in-memory and on-disk, influences performance.

\subsection{Architecture}\label{sec:architectural-costs}

An important goal of the Envoy design is to localise data and metadata control when possible, and minimise the involvement of disinterested nodes when not possible. Private images are always controlled by the envoy instance on the same machine as the client using it, and territories in shared images are managed by the most active participant. Besides reducing collateral impact in a bid to improve scalability, localising control and caching has direct performance benefits for clients.

As described in \secref{sec:data-paths}, requests follow one of several paths to completion depending on whether or not the required data is cached and whether ownership is local or remote. The impact of network and virtual machine layout is measured here, with four layouts. \emph{druid-0} is an administrative virtual machine that hosts all server processes, and owns all territories in the case of Envoy. \emph{skiing-0} is an administrative VM on a different machine, hosting Envoy server processes but no others. \emph{druid-1} and \emph{skiing-1} are client VMs on the two machines. These tests focus on hot cache results to prevent disk access time from masking other sources of overhead.

For the 9p and NFS results, druid-0 acts as the server in all instances, and results are included for all four client locations. Envoy clients attach to their local envoy instance in each case, so for clients on skiing, all requests are forwarded on to the envoy instance running on druid-0.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/arch-tar-hot}
\caption[\texttt{tar} benchmark results with a hot cache]{Time to \texttt{tar} the Linux kernel source tree with a hot cache over various data paths. NFS results are included for a hot client-side cache and for a hot server-side cache only (labelled nfs-warm). Error bars show the standard deviation over ten runs.}
\label{fig:arch-tar-hot}
\end{figure}

\figref{fig:arch-tar-hot} shows the time required to \texttt{tar} the Linux kernel source tree from each client location with a hot cache. In each case, the file data is available from the buffer cache of druid-0, so the differences are largely due to network, protocol, and implementation overhead. The Envoy and 9p results show a significant difference between druid-0 and druid-1 results, which comes from the virtual network device that connects the two VMs. Much of this overhead could be eliminated through a Xen-specific transport between the VMs combined with optimisations in the 9p driver to eliminate data copying, as suggested in \secref{sec:data-paths-read}.

Envoy is consistently slower in this test than in the Bonnie read test, which measured bulk operations from a single file. The \texttt{tar} test exercises metadata operations much more heavily, which exposes the extra overhead of a prototype-quality implementation of Envoy. For the skiing-0 and skiing-1 results, requests are considered and forwarded at the skiing-0 envoy instance, so overhead is inherent to the design. For druid-0 and druid-1, however, both 9p and Envoy are using the same protocol to deliver the same data across the same network paths, and the data is coming from the buffer cache in each instance. Optimisation of the prototype would probably result in something closer to parity between these results.

The NFS results for druid-0 are an exception caused by the client-side cache. Because the client and server must compete for cache space, the cache is not sufficient to hold the file tree and disk access is required. While seemingly an unfair circumstance for the NFS results, this result is included to illustrate the benefit of shared caching. Allowing a larger unified cache on each host instead of having each client provide its own smaller cache reduces this kind of cache duplication and increases the effective cache space available cluster-wide. While the local cache can improve performance, it can also cause more disk access and reduce both performance and the overall transaction capacity of the cluster.

A second anomaly caused by the client-side cache is the result for skiing-0, where the entire file tree fits in the client-side cache. The displayed result is only applicable when the test is repeated within the 30-second window in which the Linux NFS client considers data valid. The test completes in an average of 22.38 seconds and makes this possible, but if a delay is introduced between each iteration, the average time nearly doubles to 44.05 as each cache entry must be validated against the server.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/arch-tar-warm}
\caption[\texttt{tar} benchmark results with a warm cache]{Time to \texttt{tar} the Linux kernel source tree from the persistent cache (for Envoy), with the in-memory cache cold for all servers. Error bars show the standard deviation over ten runs.}
\label{fig:arch-tar-warm}
\end{figure}

\figref{fig:arch-tar-warm} supplements these results with the same tests with the in-memory cache flushed before each iteration. Data is resident in the persistent cache in the case of Envoy, so for each of the three servers the data is coming from the same disk. NFS suffers from the same double-buffering issue on druid-0, but otherwise its client-side cache proves very helpful for the metadata-intensive operations in this test. This is clearest in the jump to a remote machine, where 9p must consult a remote server for many operations that are absorbed by the client-side cache in NFS. If the optimisations already suggested prove insufficient, an approach like that of NFSv4---where a limited token-passing scheme lets clients access unshared files without fear of conflicts but reverts control of shared files to the server \cite{shepler}---may be useful for Envoy, allowing it to retain strong consistency guarantees while allowing local caching for unshared files. Testing with a representative workload on an optimised implementation of Envoy would better inform such a choice.

These tests do show a clear performance benefit to coupling control of territories with the clients that use them, particularly in the difference between Envoy results for druid-1 and skiing-1. Comparing the two graphs also shows that Envoy is faster serving warm data controlled by the same machine than hot data from a remote machine, suggesting that territory boundaries can be relatively fluid without destroying performance.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/arch-rsync-tar-hot}
\caption[\texttt{tar} and \texttt{rsync} benchmark results with a hot cache]{Time to read the Linux kernel source tree using \texttt{tar} and \texttt{rsync} after priming the in-memory cache with all file data. Error bars show the standard deviation over ten runs.}
\label{fig:arch-rsync-tar-hot}
\end{figure}

\figref{fig:arch-rsync-tar-hot} compares using \texttt{tar} and \texttt{rsync} to read the Linux kernel source tree from cache using Envoy and 9p. \texttt{rsync} reads asynchronously from multiple files at one time, whereas \texttt{tar} reads one file to completion before starting the next. The increase in time required for both tests between druid-1 and skiing-1---which are the two important client locations---is reasonably consistent. Envoy sees a time increase of 88\% for \texttt{tar} and 85\% for \texttt{rsync}, while 9p sees 67\% for \texttt{tar} and 63\% for \texttt{rsync}.

Connecting skiing-1 directly to druid-0 instead of to the envoy instance on skiing-0 yields increases of 40\% for both tests over the respective druid-1 results, or a reduction of 34\% and 33\% compared to the same tests using the envoy on skiing-0. The average times---168.97 for \texttt{tar} and 118.71 for \texttt{rsync}---compare more favourably to the simpler 9p server, taking 8\% longer for both tests.

Envoy uses the 9p protocol to connect to clients, and it also uses an extended version of it between envoy instances. Comparisons between Envoy and a simple 9p server using similar data paths and cached results demonstrates that the 9p server is consistently faster, but performance degrades at a similar rate for both systems as obstacles are introduced to the data paths. The 9p server passes most requests directly to the file system, while Envoy is a more complex system that tracks additional state and offers more features. Nevertheless, the correlation between the two optimistically suggests that the Envoy prototype could optimised to a performance level close to that of the 9p server without introducing any architectural changes. A roughly 33\% overhead beyond network costs for remote transactions is imposed by the forwarding system, but this is a reasonable tradeoff for simplified recovery and the possibility of removing forwarding overhead completely through territory migration. Any consistent system will require coordination for overlapping requests; Envoy's territory scheme ensures that sharing can be coordinated from an in-memory cache.

\subsection{Cache}

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/envoy-cold-vs-nocache}
\caption[Cold cache results with and without persistent cache]{Cold cache read and write tests with the persistent cache enabled and disabled. \texttt{tar} times are faster with the cache, even though it requires writing all data to the local disk. \texttt{untar}, the write test, is faster with the cache disabled.}
\label{fig:envoy-cold-vs-nocache}
\end{figure}

In addition to the data paths imposed by the architecture, Envoy requests interact with different cache states. When the cache is \emph{hot}, data is available from the in-memory cache on the envoy that owns the relevant territory. A \emph{warm} cache holds data in the local on-disk cache, but not the in-memory cache. When data must be retrieved from the storage layer, the cache is said to be \emph{cold}, and for all tests the storage servers themselves were restarted and data partitions remounted to empty in-memory buffers.

For all Envoy tests, the server was restarted between each iteration, reseting the server's internal state. For both hot and warm cache tests, the server must initially validate cached objects against the storage servers, but can then proceed to use them from the local cache. Testing found this to have little effect on the results, so all figures presented here use the more conservative method.

The persistent cache stores objects on the same node as the envoy that uses them, which is generally a performance win. For write operations, however, it simply adds another write destination for data that is already being sent to multiple storage nodes. The \texttt{untar} part of \figref{fig:envoy-cold-vs-nocache} shows modest overhead for write operations, since the writes proceed in parallel with those on the storage nodes.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/envoy-nocache}
\caption[Cold and hot storage servers with no persistent cache]{\texttt{tar} and \texttt{rsync} tests run with the persistent cache disabled. The cache status (hot or cold) refers to the in-memory cache on the storage servers. Error bars show the standard deviation over ten runs.}
\label{fig:envoy-nocache}
\end{figure}

Reads in the \texttt{tar} test are faster with the cache, even though all data must come from the storage layer in either case, and enabling the cache requires writing everything to the local disk in addition to returning it to the client. This is because the disk is otherwise idle, and most of the write operations can be absorbed by the buffer cache and proceed asynchronously. In return, having the data in memory on the local node prevents read operations from being directed back to the storage layer. The in-memory cache is provided by the host operating system through operations on the object store, so disabling the persistent cache disables the in-memory cache as well. Even though the test is primarily based on write operations, disabling the cache results in a 41\% increase in the number of messages sent to the storage layer in this test.

Even with the persistent cache disabled, there are still other cache factors to consider. \figref{fig:envoy-nocache} examines read test results with the persistent cache disabled, varying the status of the in-memory cache on the storage servers. As expected, a hot cache improves performance considerably. The improvement in cold cache results moving from \texttt{tar} to \texttt{rsync} is particularly dramatic, because \texttt{rsync} is able to schedule reads from both storage servers at once, whereas \texttt{tar} leaves one completely idle while waiting for the other.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/envoy-tar}
\caption[The \texttt{tar} test with all cache modes]{\texttt{tar} test results for hot, warm, and cold cache states on all client locations. Error bars show the standard deviation over ten runs.}
\label{fig:envoy-tar}
\end{figure}

\figref{fig:envoy-tar} compares the \texttt{tar} test run on Envoy's three basic cache states. As demonstrated earlier when the persistent cache was disabled, many requests are absorbed by the in-memory cache even when it starts out empty. Micro-benchmarks would probably show a greater difference between the three cache states, but reading a series of files is a common and realistic workload. In addition to the performance gained from caching objects on each host, the load on storage servers is reduced. Particularly when storage servers and envoy servers are hosted on the same nodes, localising traffic as much as possible helps both performance and scalability.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/envoy-rsync}
\caption[The \texttt{rsync} test with all cache modes]{\texttt{rsync} test results for hot, warm, and cold cache states on all client locations. Error bars show the standard deviation over ten runs.}
\label{fig:envoy-rsync}
\end{figure}

A cold cache is at less of a disadvantage over a warm cache using \texttt{rsync}, as demonstrated in \figref{fig:envoy-rsync}. Overlapping requests employ both storage servers in parallel, making better use of additional disk spindles. As with other tests, there is a clear advantage to accessing data on the local node. In the \texttt{rsync} results, a hot cache on a remote client (skiing-1) is faster than a cold cache on a local client (druid-1), but not by a wide margin. In the more common hot and warm cache scenarios, network latency and the cost of forwarding requests outweigh differences in cache performance.

Many cluster file systems use parallel transfers to increase throughput on large file transfers. Nothing prevents Envoy from doing so except for the current implementation. The Envoy prototype does transfer large files from all available storage servers in parallel when transferring them to the local persistent cache, but it transfers entire objects at file open time before answering any read requests. Because of this simplistic implementation, parallel reads serve to reduce latency at file open time rather than increase throughput at file read time.

If the transferred file fits in the buffer cache, then reads that commence after the transfer completes will find a hot cache and proceed quickly, but large files will incur an unfortunate performance penalty, particularly in the common patter of sequential file reads. In addition to the lag of transferring the entire file to the local node, clients will have to wait for much of the file to be written to the local object store, and then sequential reads will create congestion on the disk as the file is read back into memory and forces the remaining dirty blocks to be written to disk. The throughput advantage of striped reads will be completely lost.

A more refined implementation could cache partial results, and feed striped reads from the storage layer back to the client in real time, perhaps issuing read-ahead requests to keep the queue full. Similarly, reads that saturate the local disk could be supplemented with streamed reads from the storage layer, making use of multiple disks. More sophisticated write policies could also improve storage layer performance. With a large persistent cache at each node, a log-structured storage layer may be profitable, since write requests are likely to dominate storage layer traffic. Log-structured systems suffer under certain workloads, particularly when application semantics demand frequent checkpointing. With Envoy, writes are considered stable once they have reached enough storage nodes, so checkpointing would not need to interfere with the storage layer's bulk write policies as they do on local file systems. The results examined here show clear advantages to the simplistic persistent cache implemented in the prototype, but it leaves much room for improvement in conjunction with a fully-developed storage layer.

Client-side caching was advantageous to NFS in some tests, but proved little benefit in others. Envoy, which eschews client-side caching in favour of a larger shared cache at the machine-level, does surprisingly well in many tests despite the increased traffic to the envoy service. Some form of client-side caching would be essential for a production system, particularly when hosting shared libraries and other memory-mapped files. Achieving the right balance between cluster-wide cache capacity and performance at local nodes will require further research.

\section{Shared images}\label{sec:shared-images}

When file system images are shared, Envoy manages territory boundaries dynamically as described in \secref{sec:territory-management}. Territories are split and merged in response to runtime demand, with control being granted to the machine that is generating the most traffic for a portion of the file tree. When the current owner generates slightly less traffic than a remote node, it waits between changes for a time proportional to the degree of imbalance in request traffic. Severely skewed loads trigger more rapid changes, while minor sub-optimalities are tolerated longer to establish a longer-term trend.

The dynamic algorithm is configured by specifying the two endpoints of the urgency/delay line. The minimum delay between migrations is paired with the minimum traffic imbalance that will trigger a migration after waiting out that delay. At the other end, the smallest imbalance that will ever prompt a change is paired with its minimum delay. Urgency is defined by the number of requests to a region of the territory, decayed over time with a configured half-life.

Determining suitable values for these parameters requires knowledge of workloads and the demands of the specific environment. If stability is valued over a fine-grained territory layout, the system can be made more reluctant to enact changes. If workloads change more rapidly, a more eager policy can be adopted. The previous section compared the performance of local and remote clients, and concluded that local control yields a performance advantage as expected. Private images are always ceded to the client's envoy at mount time, so it is only in the presence of workloads with actual sharing that the dynamic algorithm comes into play.

The previous section explored the performance of the Envoy prototype under various condition, and this section turns to dynamic behaviour under shared conditions. Migrating active state between nodes is a quick operation, similar to the metadata operations discussed in the next section. The prototype also completes the operation that triggers a territory change before executing the transfer, so any pause happens between client requests. Write operations are always committed to the storage layer before returning, so no cache flush needs to take place, either. Cache effects will create some extra delays as the new owner loads active objects that were already in memory on the previous host, but the results of the previous section suggest that these will largely be offset by the improvements derived from local control.

\subsection{Dynamic behaviour}

A series of typical sharing scenarios were laid out and simulated, and the response of the dynamic territory algorithm observed. For the tests, a half-life of five seconds was used to give a gradual and predictable response to simulated traffic. An urgency of 100 triggers a change after five seconds, while a modest threshold of five triggers a change after 120 seconds.

To generate load, a script walks to and opens files at random within a subtree of the Linux kernel sources at a specified rate. Each operation sends multiple requests to the server, walking from the root of the tree (the Linux 9p driver generates a new requests for each step in the directory traversal), opening the file, and immediately closing it. The script can generate load in multiple trees and can ignore subtrees within a target region.

The prototype is instrumented to output a report after each transfer, including a graph of the complete territory structure at each end of the transfer. After being combined and plotted using the \texttt{graphviz} graph-drawing package \cite{gansner}, the output represents a live snapshot of the territories and active claims in the running system.

\subsubsection{Home directories}

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/territory-home}
\caption[Territory graph for home directory sharing]{The territory graph when sharing an image divided into home directories. Rectangles denote territory roots, and colours indicate the host that owns a directory. druid (red) is the root server and directs its requests to \texttt{char}, skiing (blue) accesses \texttt{block}, and moonraider (green) sends requests to \texttt{net} and \texttt{usb}. The tree on the left depicts the setup, while the one on the right shows the actual layout created by Envoy.}
\label{fig:territory-home}
\end{figure}

One common pattern in using existing file servers is to share a file system divided into home directories. While multiple users share a single image, in practice their activities are largely partitioned into specific subdirectories. As the root server and host to the first client to mount the image, druid-0 started out owning the entire territory. Four directories within the Linux tree are assigned to clients on the three servers, which start opening two randomly chosen files per second in each directory. The tree on the left of \figref{fig:territory-home} depicts the territory tree as planned, with the four directories at the bottom acting as home directories. The tree on the right is the actual result from testing this configuration on Envoy.

While druid had its own client traffic, it also had requests coming at an equal rate from skiing and twice as many from moonraider, which was assigned two directories. The envoy on druid had the options of ceding control of one directory to skiing and the other two to moonraider, or transferring control of the whole image to moonraider and allowing it to make further splits. The imbalance as viewed at the root of the image would be equal to that at the root of each target directory, except that each client request traversed from the root of the tree. The extra overlapping traffic near the root tipped the balance in favour of ceding control of the whole territory to moonraider, which then waited until enough time had elapsed to transfer control of the two remotely-accessed directories to their respective hosts.

The order in which transfers occur affects the final layout. Had moonraider introduced its requests one directory at a time, druid would have retained control of the top-level directories. The greedy algorithm used in Envoy does not guarantee optimal layouts, but it does try to produce useful behaviour with simple rules.

\subsubsection{Log files}

Directory boundaries are not always the ideal place to partition shared files. Multiple clients may also access individual files that are stored in a common directory. A common example of this is in log files that collect data and results from multiple servers in a shared directory. For this situation, there is no sharing within the files themselves.

\begin{figure}[t]
\centering
\includegraphics[width=\figwidth]{figures/territory-logs}
\caption[Territory graph for log files in a shared directory]{The territory graph when sharing a directory hosting privately-accessed files. Rectangles denote territory roots, and colours indicate the host that owns a directory. Clients on each of the three servers were given three files each within one directory to access at a constant rate. The tree shows the final steady-state result produced by Envoy.}
\label{fig:territory-logs}
\end{figure}

\figref{fig:territory-logs} shows the result of this test, where three files were assigned to each client. It took longer to achieve the final layout with this test for two reasons. Instead of accessing random files in a directory, clients accessed individual files, so fewer requests were generated. Navigation steps leading to the file went through shared directories and did not contribute to the urgency of transfers. In a more realistic setting, the client would likely hold an open handle for each file yielding a similar effect, so this difference is a artifact of the test script. The second reason is that more transfers had to take place, with enforced minimum delays between each. druid was the only host to initiate transfers, and each one involved the same root territory, so the process was completely sequential.

After reaching a steady state, no further traffic between envoys would be necessary if each client maintained an open handle to each file. True to Envoy's goal, having the files in the same directory would be convenient but not prevent individual machines from handling file traffic locally. With no actual sharing taking place and a consistent traffic load, no overhead for coordination is necessary.

\subsubsection{Sharing files}

The next test involves actual sharing between clients. All three hosts accessed random files within the same directory at the same rate. The expected result would be to have no territory changes, as no remote client would generate more traffic than the initial owner. An artifact of the implementation makes this dependent on the configuration parameters. Directory navigation results are cached on each envoy, even for remotely-owned territories. To minimise stale cache effects, the final target of each 9p \texttt{walk} requests is contacted directly if it is remote, even when a cache entry exists. Since the Linux 9p driver issues a separate \texttt{walk} request for each directory, this means that the navigation cache is effectively disabled for remote hosts. It is active for locally-owned territories, however, which effectively reduces the number of steps for local clients but not for remote clients. Because of this, remote clients appear to generate more requests despite the uniformity that is carefully-maintained by the clients.

With a slight urgency boost for remote clients, an equal sharing situation can have one of two effects on territory boundaries. If the idle threshold is high enough, the extra navigation traffic generated by the test will not be enough to trigger a transfer, and control will remain with the initial owner indefinitely. If the idle value is too small, however, control will eventually be passed to one of the remote hosts, which will then face the same situation. Since this thrashing happens on a slow time scale (nearly 120 seconds between transfers in this particular test), the detrimental effects on caching are minimal. One might argue that periodically passing control between equally active clients is ultimately fairer, if not an absolute win in terms of system-wide performance and capacity.

\section{Image operations}\label{sec:image-operations}

Service clusters must be able to easily manipulate file system images to achieve their promised flexibility. Providing templates with commodity software and allowing clients to easily clone and customise them makes service instantiation a relatively lightweight operation. When combined with functionality in the virtual machine monitor to suspend, clone, and resume running services, this could allow rapid instantiation of common services. Envoy provides support for rapid forking of file system images, along with the ability to take snapshots of active images. This section measures the performance of these two management operations, along with the time and space required to deploy new services.

\subsection{Forks}

New images can be created from scratch, or they can be forked from an existing image. Forks always start from a snapshot of an existing image, and snapshots are always read-only. With the copy-on-write mechanism described in \secref{sec:snapshots}, this makes forks a fast and efficient operation. Even if the template snapshot is in active use, only the root of the image needs to be copied to isolate the new image from the old.

The time required to fork an image was measured in two ways. In the first, the server was instrumented to record the time spent completing the operation. Over fifty iterations, the average fork took 339~$\mu$seconds with a standard deviation of 32~$\mu$seconds. Over the same fifty iterations, the client VM observed the time required to fork the template and create a new file in the new image. \texttt{/usr/bin/time} reported 10~milliseconds for each iteration, which corresponds to its timer resolution.

\subsection{Snapshots}

Forking an existing template is fast because few operations are involved. Snapshots can be taken of active images, which may involved cloning storage objects as well as copying runtime state. If an image has been divided into multiple territories, the snapshot operation will span multiple envoy instances and will required cloning a path from the root of the image to each territory root, as discussed in \secref{sec:snapshots}. Forking an active image also involves taking a snapshot and then performing a fork operation, so dividing an active system is dependent on the snapshot mechanism as well.

In the simple case, snapshots are nearly as fast as forks. When an image is inactive, taking a snapshot requires cloning the root, adding the new snapshot to the parent directory, and updating the link to the active view of the image. Over fifty iterations, an instrumented server reported an average of 1258~$\mu$seconds per snapshot with a standard deviation of 108~$\mu$seconds.

To evaluate more complex snapshot cases, a tree of territories involving three machines was created. Control of a Linux kernel source tree was divided such that every directory was the root of a territory owned by a different envoy than its parent, i.e., every territory was forced to cede control of subdirectories equally between the other two servers. druid-0 was the root server and hosted 439 territories, skiing-0 hosted 421, and moonraider-0 hosted 379 territories, for a total of 1239 territories. This is far beyond what one would expect in a normal deployment for a single image, but serves to exercise the snapshot mechanism under extreme conditions.

After constructing the territory tree, fifty snapshots were taken of the kernel image in an average of 13.71~seconds each with a standard deviation of 3.13~seconds, or approximately 11.1~milliseconds per territory in the tree. Most of this time was spent cloning objects to ensure that the root of each territory was writable in the active image after the snapshot completed. Under Envoy's consistency semantics, this requires submitting changes to the storage servers before considering the operation complete, but not flushing their buffers to disk. If the snapshots are done in rapid succession, the storage server buffers cannot absorb the traffic and disk write times increase the time. After a warmup run of several snapshots, fifty more were done without pause in an average of 73.13~seconds each with a standard deviation of 26.8~seconds, or approximately 59~milliseconds per territory. The high variability comes from the lack of coordination between the client operations and the buffer cache on the storage serves.

The snapshot timings reflect a combination of network messages, internal state updates, and storage object updates. The bulk of the time is spent doing storage object updates, though this is not measured directly. To approximate this, another operation can be measured that performs similar state updates and has the same network message profile, but does not involve updating storage objects. Renaming the root directory requires recursively notifying child territories of the pathname change, which makes it a suitable choice. Fifty such operations complete in an average of 96.7~milliseconds each with a standard deviation of 1.09~milliseconds, approximately 78~$\mu$seconds per territory.

\subsection{Service deployment}

Besides making service deployment quick, an important goal of snapshot and fork operations using copy-on-write is to encourage the use of commodity software and exploit the redundancy that results. Objects inherited from a common template or a previous snapshot can be read by any envoy without coordination, as higher-level semantics guarantee that the object will not be modified again. This allows cache sharing for read-only objects between unrelated services. Besides cutting down the cache required on a given node, this makes it likely that common template objects will already be loaded into the persistent cache of typical nodes, allowing the most common objects to be served locally and reducing the cluster-wide load on storage servers.

\begin{table}[t]
\begin{center}
\begin{tabular}{l|>{\ttfamily}l>{\ttfamily}l}
& \textbf{\textrm{cold cache}} & \textbf{\textrm{hot cache}} \\ \hline
client requests/bytes   & 70,016/\hfill 1,264,742    & 71,648/\hfill 1,296,578 \\
client responses/bytes  & 70,016/\hfill 79,457,018   & 71,648/\hfill 81,323,281 \\
storage requests/bytes  & 2,764/\hfill 288,858       & 2,059/\hfill 285,429 \\
storage responses/bytes & 2,764/\hfill 17,476,789    & 2,059/\hfill 675,196
\end{tabular}
\end{center}
\caption[Service deployment storage overlap figures]{Messages and bytes transferred when a virtual machine boots from an Linux template image. The first was booted with a cold cache, and the second after restarting Envoy but not clearing the in-memory or on-disk cache. In the hot cache case, objects had to be validated against the storage server because of the server restart, hence the similar message counts, but data could be retrieved from the cache.}
\label{tab:deployment-stats}
\end{table}

To measure this effect, a template image was prepared with SUSE Linux Enterprise Server~10, and a virtual machine booted from a fork of this image with a cold cache. Another VM booted from a second fork of the image, this time with the cache primed from the first one. Boot times as measured with a stopwatch were about 50~seconds in either case, with the hot cache a few seconds faster (compared to about 30~seconds when booting from a local partition). Boot times were hurt by the lack of client-side cache, particularly for memory-mapped files including shared libraries. While optimisations could make node-level caching sufficient for most conventional file operations, support for executables and shared libraries is rather poor in the current cache model.

\tabref{tab:deployment-stats} shows the results collected by an instrumented version of Envoy. The server was restarted between tests, so the hot cache results include message traffic to validate cached objects with the storage servers. Booting the second image pulled less than 4\% of the bytes retrieved from the storage layer when starting the first image, showing a heavy overlap between read-only data in the two instances. Combined with the lightweight snapshot and fork operations, this suggests that service instances can be created and deployed quickly and with little impact beyond the local machine.

\section{Summary}

The prototype is evaluated on a small deployment to test and validate design decisions, to explore possible bottlenecks, and to explore the dynamic behaviour of the system.

Performance is measured on static system layouts. Envoy compares favourably with userspace NFS and 9p servers on bulk reads and writes, but is slower when more metadata operations are involved. Comparison to 9p for similar data paths suggests that significant overhead is due to an unoptimised implementation, but other costs are due to the architecture. Forwarding requests to a remote envoy costs roughly 33\% more than connecting the client directly to the remote server. Caching makes predictable improvements in read performance, though the local persistent cache hurts write performance in many cases.

In the presence of sharing, dynamic territory management works largely as predicted for common sharing cases. Shared images with privately-accessed directories and files are successfully partitioned along appropriate boundaries. When files are shared and accessed at similar rates by multiple clients, the system is reluctant to transfer ownership, though slight imbalances can trigger changes over a longer time scale.

Image-level operations are fast, with performance limited mainly by the amount of disk activity they incur. Booting multiple virtual machines from forks of a common template image demonstrates a high degree of sharing. Traffic to the storage layer for the second is only 4\% of that of the first by bytes transferred. While the lack of per-client caching is acceptable for many operations, it hurts the performance of memory-mapped shared libraries enough to significantly slow down the boot process.
