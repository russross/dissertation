\chapter{Evaluation}\label{cha:evaluation}

synthetic workloads are a hard problem \cite{ganger95}

\section{Experimental setup}

\subsection{Test machines}

\subsection{Benchmarks}

\subsubsection{Linux source tree}
untar -- write test

tar > /dev/null -- read test

rsync -- 2nd read test

\subsubsection{Bonnie}

\section{Performance}

If the goal is to get stuff as local as possible, quantify the benefits of achieving that.

\subsection{Architectural costs}

userspace NFS

userspace 9p

envoy local dom0

envoy local domU

envoy remote dom0

envoy remote domU

cache: cold vs warm vs hot

\subsection{Fancy features}
\subsubsection{Forks}

cheap and fast---these always happen from a read-only snapshot

\subsubsection{Snapshots}

single territory, many territories

untar the kernel while a bunch of snapshots happen and measure the impact

\subsubsection{Territory migration}

\section{Scalability}
\subsection{Independence of private images}
\subsection{Degradation of a single host with many clients}
Shared image, and also private images on one machine

\section{Dynamic behaviour}

Test dynamic territory management, less about performance than behaviour

\subsection{Example scenarios}

shared image with (independent) home directories

2 log files in one directory

producer-consumer

\subsection{Sharing application}
distcc or something else that shares in a complex but predictable fashion

\subsection{Synthetic workloads}

probabilistic traffic driven to overlapping areas

\section{Case studies}

\subsection{Service deployment}
show sharing: boot one VM from cold cache, boot another based on same template

\subsection{Quantifying sharing}

SUSE10 upgrade, install services

compare image overlap

boot two related images (one cold and other warm) and compare with the same test for identical images

\section{Summary}
