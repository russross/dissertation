\section{NYSE and time-based flexibility}

At 9:30~a.m.\ the New York Stock Exchange opens for trading and the computing demands of many financial services companies are focused on the fluctuations of the markets until the closing bell rings at 4:00~p.m. In the hours until the markets open again, the resources of these companies may turn to intensive jobs---such as simulations---for the company itself, or to handling the processing needs of clients. The applications and data sets involved may be completely unrelated to those used during the daytime hours.

Standard business hours drive many other computing patterns as well. A web site will have the highest traffic load when its target audience is awake and in front of a computer. For workers using personal time, this may mean a traffic surge in the morning, another around lunchtime, and a third during the early evening. For email and other services used by businesses, the traffic curve follows business hours, possibly in many time zones. In contrast, game hosts are busier during leisure hours and see many clients otherwise engaged when corporate offices open their doors.

As internet services become more sophisticated, human clients are not the only ones that drive resource usage. Mobile agents and other automated services are increasingly becoming clients as well as servers. Amazon recently unveiled a service to open their storage platform directly for use by others for web services. Major internet services like eBay and Google offer APIs for third parties to interact with their services using custom software, rather than just their usual web interfaces for human clients.

All of these examples benefit from the simple and rapid deployment and management of software services in a range of environments. Recent years have seen a renaissance of machine virtualization technology, this time in cheap, commodity hardware.

In this chapter I start by defining \emph{flexible commodity computation} and discussing some examples that motivate its adoption. I then discuss existing commodity computing systems and how they succeed at exploiting commodity systems but fail at making general-purpose computation a true commodity. I argue that the combination of virtual machine managers and clusters of commodity hardware offers a potent platform for service deployment, which I call \emph{service clusters}. I further argue that existing storage systems are not well suited for this environment when used as a platform for flexible commodity computation, so a new set of requirements and a storage system to match them are called for.

\section{Service clusters as a platform using VMs}

Hardware virtualization is one of the more potent tools available for achieving these ends. Virtual memory systems are a standard tool for isolating processes managed by operating systems, and virtualizing entire hardware platforms extends the ability to partition resources to the operating system level, which is a more flexible unit of management \cite{hand}. Once confined mainly to specialized server hardware, machine virtualization is emerging as an important general purpose management tool. The Xen hypervisor \cite{barham} was the first to make virtualization efficient for commodity hardware, especially for I/O intensive applications.

Using virtual machines makes it easier to manage OSes \cite{chen}. Gartner thinks virtualization is the way to go \cite{bittman}.

\section{Distribution}

Three basic architectures are available. A centralized server model is the simplest and satisfies the mobility requirement, but it doesn't scale well and suffers from a single point of failure. This model has been extended quite successfully using RAIDs and SANs on the back end to increase performance and robustness, and by caching on the client end to reduce load, but it is still limited. Ultimately, all decisions are moderated by a single host which must communicate directly with all clients, so at larger scale the network bandwidth becomes a problem as well.

At the other extreme, coordinations is pushed completely to the clients. This is essentially the centralized model turned upside down. Instead of many clients talking to a single server, a single client must communicate with many servers. Instead of one congestion point, every client becomes overwhelmed with bookkeeping. In practice, some structure can be introduced so that the system isn't a full $n$-way clique---the host for each object of storage may become the mediator for access to that object, for example---but making every participant a symmetric peer is mostly useful for privacy or other non-technical concerns.

A more practical approach is to emulate a centralized architecture as much as possible, since this provides the simplest model with the least administrative overhead, and carve up the problem enough to achieve the desired scaling properties. The two extremes offer the greatest conceptual purity, but the practical benefits are to be had somewhere between them. Most P2P services now introduce some kind of a hierarchy that designates some hosts as ``superpeers'' or ``supernodes'' that act as aggregation points for a manageable set of participants. These distinguished hosts only need to communicate directly with other similarly endowed hosts, potentially reducing the visible size of the network by orders of magnitude. The world is small enough and computers big enough that a few orders of magnitude is enough reduction to make many distributed problems tractable.

Another way of viewing this problem is to consider how many manually administered systems are often implemented today. NFS \cite{callaghan} is still one of the most widely-used systems, and a workstation environment may include a series of NFS servers to provide storage for a larger number of client workstations. Each server will be backed by a RAID \cite{patterson} system for improved performance and reliability, and the administrators may divide the file system namespace in order to balance average demand over the different servers. While most file access in uncontended, having a single server that serves all interested clients makes coherence easy (ignoring the effects of caching).

An interesting thought exercise is to consider the same set of workstations locking into a long-term steady-state behavior, and then asking the systems administrator to hand configure the system based on those usage patterns. One reasonable possibility would be to locate files or branches of the namespace tree used exclusively by a single workstation on that workstation. When two or more users require the same file or set of files, it could be managed by the workstation that uses it the most and exported via NFS (or a similar client-server file system protocol) to the other participants. Since contention is relatively rare, this would reduce network traffic for the storage system to a low level for as long as the hypothetical steady-state continues. One of the goals of the Envoy design is to capture this approximate topology.

In service clusters, physical hosts are divided into roughly 10s of services, which may be transient, untrusted, and unreliable. This increases the number of participants requiring access to the storage system by the same factor and exposes difficult security problems. One of the principle benefits of isolating services in individual virtual machines is in reducing the impact of services that fail, either benignly or maliciously, and trusting the administration of the file system to the services negates this benefit.

The environment includes a trusted management domain on each server, however, which introduces a natural aggregation point and a way to sidestep many of those problems.

\section{Supporting services}

Live migration of virtual machines \cite{clark} is essential for load balancing and uninterrupted service in the presence of hardware servicing. To support migration, a file system cannot be tied to a single physical machine; any given file system image must be as mobile as the service that relies on it.

One of the attractive features of running individual services in virtual machines is the flexibility of management this model offers. Services can be decoupled, instances can be created and destroyed easily in response to need, and a new service can be deployed without dedicating a new machine to it, removing an old service to make room, or studying the potential interactions that would occur if a shared server was the only option. To this end, lightweight operations are required to instantiate and clone services.

The unit of management is an entire operating system on a virtual machine. Installing an operating system for each service instance would be prohibitively slow, labour intensive, and wasteful of space. A typical installation of Linux uses hundreds of megabytes or gigabytes of space, and an automated installation is unlikely to give the right set of installed packages and running daemons, so some customization will be necessary.

Instead of trying to automate installations, Envoy offers lightweight operations to fork a file system.

\section{Overstudied storage systems}

Storage systems have been studied and documented extensively in the systems literature. One could argue that everything worth doing has been done, or alternatively one could assert that the continuing research is evidence that everyone has failed so far, with the truth probably residing somewhere between these two extremes. Storage is an essential component of most systems, though, and the requirements placed on them are almost as various as the solutions that have been proposed.

In this chapter I survey the most important and widely known storage systems, with an emphasis on those that are relevent to service clusters and similar in design to Envoy. I describe some of the major design tradeoffs that emerge and identify the characteristics of service clusters that make existing systems unsuitable or less than ideal for this emerging environment as I justify the need for another storage system.

\section{Denali and Terra}

Denali \cite{whitaker02} is also designed to support deployment of untrusted services on shared hardware, but it breaks binary compatibility to support a narrow range of services in large numbers. While suitable for some types of utility computing, it is not flexible enough for general-purpose commodity computation. Terra \cite{garfinkel} also addresses a different problem by securing guest services using hardware extensions to provide a trusted environment.

\section{Using Envoy}

This section gives a brief tour of how Envoy is used and managed to provide a general context for more detailed discussion of its features and design. Envoy is intended for a specific environment to provide for the storage needs of a specific class of problems. It relies on some features of that environment to simplify the design and optimize the implementation to match the expected demands.

\subsection{Use and administration}

This section presents the administrative interface to Envoy, which is designed as a series of special file operations and conventions.

All services share a single cluster-wide namespace tree with a common root. Mounting the root of the entire tree requires special privileges, normally reserved for the administrative tools managing the file system. The top levels of the hierarchy respond normally to standard file operations, with a few notable exceptions described here.

A few file names are given special significance and their use is restricted. The name \texttt{current} is reserved for use as the root of a client file system, positive integers are reserved as the read-only roots of snapshots (in ascending order) of the tree rooted at \texttt{current} in the same directory, and a symbolic link names \texttt{snapshot} is created and protected by the system to link to the most recently-created snapshot in the same directory. In addition, a normal file named \texttt{password} can exist to store credentials for clients accessing descendents of the containing directory.

The namespace is conceptually divided between the administrative levels and the client levels. The client levels include anything that is a descendent of a directory named \texttt{current} or one of its snapshots (named as positive integers). Any path that does not pass through one of these points is considered part of the administrative namespace and is subject to the restrictions and special semantics described in this section.

To create a new file system root, a user or management tool creates any desired layers of files and directories within the administrative area, ending with a directory called \texttt{current}. Services can then mount \texttt{current} or any of its descendent directories. A client mount request specifies the desired root pathname along with the username attempting the mount and any credentials required to validate the request. The server checks these credentials against any files called \texttt{password} that it encounters in administrative directories passed through before arriving at \texttt{current}, with those further down the hierarchy overriding those encountered earlier.

In addition to end-user mounts, this system of password files applies to the administrative file areas as well. If given suitable credentials, a service can attach to the namespace in an administrative directory to create and manage its own file system trees and credential files, creating a hierarchy for management roles as well as file storage. Controlling access to any directory requires credentials to mount its parent, along with standard file permissions within the parent directory to manipulate the credential file. Envoy does not map user and group names to numeric IDs, so any clients that can agree on usernames and credentials can share access to a file system.

The \texttt{current} directory---in addition to marking the transition from administrative to client directories--- can have its snapshot taken. When the server receives a request to create a symbolic link with a positive integer as its name and \texttt{current} as its target, it checks to make sure that the integer is correct for the next snapshot (either \texttt{1} or the number linked to by \texttt{snapshot} plus one). If so, it takes a snapshot of \texttt{current}, makes it accessible using the requested name (accessible as a directory, not a symbolic link), and creates or updates \texttt{snapshot} to link to the newly created snapshot. Any request with the wrong target or the wrong number is rejected.

Snapshots are always given positive integers as names (and cannot be renamed), but this is not always convenient for use. It may be useful to name a particular snapshot with a more meaningful name using a symbolic link. Symbolic links are normally considered opaque by the server and cannot be used in any mount or directory-change requests, so to make accessing named snapshots more convenient the server silently dereferences symbolic links that refer to snapshots in the same directory.

Snapshots also serve another special purpose: they can be used as the root for a file system fork operation. Just as the \texttt{current} directory diverges from its most recent snapshot using copy-on-write semantics, new \texttt{current} roots in other administrative directories can use an existing snapshot as a starting point. A request to create a symbolic link called \texttt{current} that refers to the fully-qualified pathname of a snapshot (again, links to a snapshot within the same directory are dereferenced, but others are not) is treated as a fork operation using the given snapshot as the starting point for a new root file system. The newly forked file system is subject to the same rules as those created with \texttt{mkdir}, and respects the credentials local to its creation site, not those of the snapshot from which it diverges. For this reason, fork operations require sufficient credentials at both locations.

\section{Lease migrations are an optimization}

Lease migrations work to optimise traffic patterns in the long term, but migrations are just that: an optimisation. They only help when there is heavy traffic or a long-term pattern, where caching can come into play. Frequent changes thrash the cache and disrupt normal operations. Migrations require coordination between envoys and may require halting normal file system traffic. We want infrequent lease changes that give good long-term behaviour rather than frequent changes that try to chase short-term trends. Since latency is short in a cluster, we don't lose all that much by going to a remote envoy for requests: this is a lot like conventional NFS servers now which seem to do just fine. One consequence of this is that lease migration operations can be straightforward and obvious without heavy optimisation, while still not hurting the overall system too much (Amdhal's law).

\section{Image credentials}

The credentials required to access a given directory are determined by the contents of files called \texttt{password} located in the administrative directories above the target directory. The access granted by each level is additive, i.e., permissions can only be added by a new \texttt{password} file, not taken away, and normally applies to all subdirectories that are siblings of the \texttt{password} file, not to the directory that contains it. The root of the entire tree is an exception, as its \texttt{password} file applies to the root directory as well as its descendents. The significance given to \texttt{password} files only applies to administrative directories. Once the boundary to an image has been crossed, all special names recognised in the administrative directories cease to have significance.

The lowest level of access granted by a \texttt{password} file is read-only or read/write access to images. This allows clients to mount images and use them, but grants no access to the administrative directories. The next level up lets the client mount administrative directories to trigger and manage snapshots, but not create new images or manipulate other files in the administrative areas. The highest level of access permits these activities as well, giving clients the ability to manipulate lower-level \texttt{password} file as well. To fork an existing image, the user must also have read permission to the complete base image.

Credentials can be restricted to grant access as a specific user name in the file system, or they can be mapped to any user within an image. The latter case is analogous to the \texttt{root} user in a Unix system, which can assume the identity of any other user within the same system.

With this system, a few typical user types are expected:

\begin{description}
\item[Service managers] coordinate groups of client VMs that cooperate to form larger services. They create new boot images, set up shared volumes, and grant appropriate access to the individual participants.

\item[Client owners] act like root users for a particular image. The client may create its own user names, but instead of registering them individually with the file system, the client owner uses its own credentials to authenticate them and grant them a name for the duration of the active session. Client owners may also manage their own snapshots.

\item[Independent users] may be authorised by service managers to access individual shared volumes within their management domain, without granting them \emph{carte blanche} or putting them under the control of individual client owners. These free agents are guests in the images they visit, but their access may come high in the file tree giving them access to many images.
\end{description}

Other roles may be created with the system. The objective is to make it simple for individual users, allow clients to manage their own user base in shared setups, and allow restricted access to be granted by clients to other clients for sharing.

\section{Storage layer}

In a centrally controlled cluster, assigning ranges of object IDs to storage servers and picking master replicas is a job best accomplished by a centralised, possibly replicated server. Likewise, coordinating bulk transfers of objects between servers to accommodate new nodes or compensate for failed machines is much easier to manage from a single location than to manage using a distributed protocol. Such tasks would be light work even when controlling many machines, because the events that require coordination occur infrequently. Unlike peer-to-peer systems, managed clusters can trust individual nodes and avoid distributing tasks that are better handled by centralised services.
