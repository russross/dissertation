\chapter{Background}\label{cha:background}

\section{Commodity computing}

Commoditization relates to computing in two distinct ways, both of which are relevant here. The first is the general consolidation of PC hardware into a series of commodity parts, which are cheap enough and powerful enough to take over many applications that were the domain of specialized ``big-iron'' hardware in the past. The second is the push to offer computation as a service instead of as a product, where billing is based on work done rather than on hardware and software delivered. This dissertation proposes using clusters of commodity hardware as a platform for offering commodity computation services.

\subsection{Commodity big iron}

The scale of the PC market ensures that the best the semiconductor market has to offer is available for PCs. The difference between cheap commodity hardware and expensive mainframes and supercomputers is in scalability, reliability, and manageability. High-end equipment is designed to offer high bandwidth on buses and I/O channels and to scale with highly-parallel configurations. Redundant components mask some classes of faults, and others can be isolated and repaired without interrupting the operation of unaffected components. While these features could be offered in commodity machines, they add too much cost to be worthwhile in environments where service interruption is more annoying than expensive.

\subsubsection{Networks of workstations}

The proposal for Networks of Workstations was an early argument for using networks of smaller computers for large problems instead of seeking parallelism through specialized hardware \cite{anderson95a}. While individual components are less performant, the aggregate capacity of a large collection of machines is immense. Commodity disks may be slow, but accessing them in parallel yields high bandwidth as well as high capacity. Reliability and fault tolerance can be designed as a property of the combined system, rather than being engineered into individual hardware components. This has the advantage of transferring the expense from a per-unit manufacturing overhead to a one-time cost in the software design.

PCs have encroached on the workstation market and the two terms are largely synonymous now. The same trends that prevailed in the mid~90s for workstations continue today, however. Switched networks lead to aggregate bandwidth that scales with the number of hosts. Single-threaded performance on cheap PCs is a significant fraction of that on the fastest server hardware. Disks continue to grow in capacity, but their performance improves at a muct slower rate. Performance is best scaled through parallelism, so a collection of many machines has the potential to outperform even the fastest single controller or collection of disks attached to a single machine.

\subsubsection{Clusters}

While workstations in an organization can be profitably pooled for large jobs, clusters are specially built for that purpose and eliminate the workstation role from a node's list of duties. The basic principles behind clusters were laid down in the 1960s \cite{amdahl}, but the power and affordability of recent commodity hardware have made brought clusters to renewed prominence. The same hardware used in high-street PCs powers some of the most powerful computing sites in the world \cite{top500}.

Dedicated clusters have a few advantages over more loosely-organized structures. Nodes are typically housed in a controlled environment with redundant power supplies, network links, and cooling equipment. While many clusters grow incrementally, they are still relatively homogenous, being built from batches of identical hardware. They are also professionally managed and monitored, with central control over all resources.

As parallel computing vehicles, clusters are mainly used for large ``embarrassingly parallel''\footnote{See \url{http://en.wikipedia.org/wiki/Embarrassingly_parallel}} problems that can be neatly decomposed into discrete tasks. Google has been prominent among commercial users of commodity clusters, and their search engine architucture is an example of a highly-parallel task. The search index can be cloned to serve many concurrent transactions, and partitioned to split processing for a single query over multiple nodes \cite{barroso}. Scientific workloads, simulations, and graphics rendering farms are all characterized by repetitive computations over huge data sets, and are similarly well-suited to cluster implementation. Beowulf is a popular system specifically designed for addressing these kinds of tasks on clusters of commodity hardware running Linux or other Unix-like operating systems \cite{ridge}.

These problems are generally large enough to warrant custom software solutions as well as dedicated hardware resources. Monitoring and management software for clusters usually assumes that the application is trusted and cooperative. Systems like TranSend \cite{fox} and CARD \cite{anderson97} require application software to be divided into discrete units that the monitoring software can direct. Migrating tasks from node to node and compensating for failed nodes requires cooperation from the application, or interaction with a transaction interface that exposes application semantics.

For some classes of problems, clusters dominate as the architecture of choice, with clusters of commodity hardware increasingly winning over clusters of specialized hardware. This disseration does not seek to supplant them for parallel tasks, but rather to extend their advantages into other problem domains. While clusters are a sound choice for problems requiring the dedicated effort of thousands of machines, they can be an equally viable platform for problems too small to require the undivided attention of even a single node. Clusters are normally built and located for a specific user and a specific task, but they can also be built where conditions are best, perhaps near network interconnect points or where cheap resources abound or a favorable regulatory environment exists; strategic choices can give vendors competive advantages. All the advantages of scale can be brought to bear on small and medium-sized problems, not just on the largest of tasks.

\subsection{Computation for hire}

\subsubsection{Replacing the machine room}

Clusters not helpful for low-volume stuff that still needs a server. email, payroll, calendar, game server.

Hosted services are usually about the vendor: what can they do to drive business their way and keep it. Instead of picking the most lucrative services and offering them in full, how can we get rid of the machine room altogether and outsource the entire computational environment. This isn't about getting rid of desktops, it's more about database-driven apps, network services, computationally intensive tasks, and sporadic/intermittent demands. Most stuff that runs in the back room would be better on a service cluster somewhere. Cheaper, better connected, no more hardware worries, less staff.


\subsubsection{Grid}

Similar to clusters, but focused on harvesting cycles from a huge area. Big, parallel problems, complex setup, custom tools. Mainly for CPU-intensive jobs.

Volunteer projects

Commercial and research projects

PlanetLab

electricity/running costs are higher than capitol costs, so value is dubious

\cite{foster}
\cite{figueiredo03}

\cite{zhao04}

\subsubsection{Utility computing}

Utilification \cite{wilkes04}.

\subsubsection{Hosted services}

Web services: Why are these attractive? Many of the same reasons commodity computation is desirable: simple way to outsource IT work, continuous upgrades, accessible from anywhere. We want all these things but more general purpose; web services should be built on service clusters, letting providers specialize even more.

email, payroll, tax prep

Storage outsourcing makes sense \cite{ng}.


Similar idea as network appliances: \cite{sapuntzakis03}.

SoftUDC: software-based data center \cite{kallahalla}.

\section{Virtualization}

Resource virtualization makes it possible to put familiar tools and interfaces in new environments without substantial modifications. Virtual machine abstractions allow standard operating systems to share hardware even when they are designed to assume exclusive control. Virtual disk abstractions provide a block interface like that of a standard disk, but without tying it directly to a single piece of hardware. Virtualization is used for a variety of reasons, but the relevant purpose here is for resource management and sharing.

\subsection{Virtual machine monitors}

IBM VM/370 \cite{gum} first virtualized hardware to support legacy code without modification. Modern commercial systems like VMWare \cite{vmware} and Microsoft Virtual Server \cite{microsoftvm} are often used for similar purposes, allowing standard operating systems to run unmodified as guests in new environments. This is useful for testing labs, which can test applications against a range of software configurations on limited hardware, and for end users who need access to applications from multiple software platforms. While this allows access to a wide range of guest software, it comes at a performance cost on some hardware, including the standard x86 platform, which was not designed with virtualization in mind.

Virtual machine monitors (VMM) work by imposing a thin software layer between the operating system and the bare hardware. Unlike emulators or simulators, VMMs allow code to execute directly on the hardware whenever possible. Certain operations break the illusion of isolation, however, particularly those involving I/O, memory protection, and interrupt management. The VMM must intercept these operations and simulate the desired effects on the virtual machine. If the hardware does not allow the VMM to directly intercept such operations, it must do so using software means, such as code rewriting or emulation. The overhead of such virtualization techniques is highest when I/O operations are frequency, as is common in server applications.

The Xen hypervisor uses \emph{paravirtualization}, where the operating system is explicitly ported to the hypervisor environment. By eliminating expensive virtualization techniques for I/O operations, paravirtualization leads to low overhead even for network and database applications with heavy I/O components \cite{barham}. This does not affect userspace applications, whose I/O operations are already virtualized by the operating system, so unmodified binaries can still be used with the modified operating system. The down side is that paravirtualization does not provide support for systems that are not modified to support it, including legacy systems, but it does support modern commodity systems that have been ported. The combination of high performance and support for modern commodity systems makes it suitable as a host for a managed platform like the one proposed here. Recent updates to the x86 platform support full hardware virtualization, allowing unported systems to run as well, although explicit support for the paravirtualized environment is still beneficial in reducing overhead.

% Denali \cite{whitaker02} is also designed to support deployment of untrusted services on shared hardware, but it breaks binary compatibility to support a narrow range of services in large numbers. While suitable for some types of utility computing, it is not flexible enough for general-purpose commodity computation. Terra \cite{garfinkel} also addresses a different problem by securing guest services using hardware extensions to provide a trusted environment.

Virtualization allows a commodity computation platform to isolate hosted services from each other and allow them to be managed without requiring explicit cooperation at the application level. Multiple guest operating systems can run on the same hardware, and virtual machines can be migrated transparently to balance load, co-locate related services, or free a machine for servicing \cite{clark}. Virtual machines can even be frozen and migrated to remote sites \cite{sapuntzakis02}, a deployment strategy used by vMatrix \cite{awadallah} to migrate and replicate internet services.

The IBM zSeries \cite{ibmzseries} platform provides a managed hosting environment for paravirtualized Linux, but it does so on specialized mainframe hardware and ties clients to a single vendor solution. The availability of fast, secure virtualization for cheap commodity hardware opens up the possibility of applying these techniques to create an open commodity market for computation.

\subsection{XenoServers}

A public computing platform requires more than just hosting technology. The XenoServers project \cite{reed,kotsovinos04a} explored the infrastructure required to host network services on a public computing platform. The Xen hypervisor started as part of the XenoServers project, which motivated its emphasis on performance over compatibility with legacy code. XenoSearch \cite{spence} allows servers to be located and selected based on connectivity requirements as well as availability, an important factor for network services.

Lots in common---generally complementary. Service clusters are the ideal platform for XenoServers. XenoServers lacks a storage story, and its layout is too much like PlanetLab---a few servers here and there, with the bonus of making some money on the side. All the benefits of scale need big clusters and specialization.

\cite{kotsovinos04b}


\section{Storage systems}

Storage is a fundamental component of all general-purpose computer systems. A combination of high storage density, random access, and low cost have made magnetic storage on rotating platters the dominant medium for durable storage. The time to access a random byte of data from a disk is typically measured in milliseconds, while for DRAM this time drops roughly six orders of magnitude to a nanosecond scale, and that gap is continuing to grow.

Because disks are so slow, storage systems are designed around the goal of accessing the disk as little as possible, and favoring sequential access to avoid costly seek delays. Effective use of cache is vital to this goal, and specialized storage systems tuned to specific system architectures and expected workloads are worthwhile because of the potential speed gains over more general systems.

Storage systems have been studied extensively, and one purpose of this section is to survey related work that has influenced this dissertation. A second purpose relates to the thesis of this work, which argues that a new environment justifies a new storage architecture. The works discussed here are presented in the context of the environments that inspired them, in part to establish a pattern of symbiosis between storage architectures and computation environments, a pattern at the heart of the present work. Envoy, the file system introduced in this dissertation, is discussed briefly in this section to relate it to prior work, but details of its design are reserved for later chapters.

\subsection{Local file systems}

Disks are mechanical devices and their performance is limited primarily by the need to physically position the disk head over the correct part of the platter to access data. File systems designed for local disks achieve performance by minimizing the physical movement required. Disk access can be reduced through caching, and hardware latency can be minimized by arranging data on the disk to minimize movement. Correctness and reliability tend to trump performance as design considerations, however, and the wide variety of possible workloads makes it difficult to pick clear winners from competing designs.

The Berkeley Fast File System (FFS) was the first to optimize data layout for performance by clustering related information. Block metadata is distributed across the disk to be near the file data it describes, file metadata is grouped for files in the same directory, and blocks in the same file are grouped whenever possible \cite{mckusick}. \emph{Clustering}, as this design is called, exploits concepts that extend beyond local file systems: the same expectations of correlated access can be exploited to reduce latency when accessing data across the network.

File system tracing reveals how files are accessed in real systems \cite{ousterhout}. Most files accessed are small, but size distributions are skewed enough that most data transferred is from large files. Writes are less common than reads, and most files are short-lived. Files access is typically sequential, and most files are read from or written to in their entirety. These trends have proved resiliant over time, though the scale has increased and largest files in typical systems have grown much larger \cite{ruemmler,gibson98b}.

As cache sizes grow, more requests can be satisfied without consulting the disk, and designs can assume that many read requests can be served from memory. All writes whose effects are not quickly undone have to go to disk eventually, so despite being fewer overall, writes requests can dominate the mix of operations that penetrates the cache and reaches the disk. Furthermore, updates often involve metadata changes as well, potentially requiring multiple costly disk seeks even for small updates.

The log-structured file system (LFS) addressed this problem by borrowing from database design and making the entire file system an append-only log. Writes are gathered and written as sequential chunks on disk, with relevant metadata rewritten instead of updating existing structures directly \cite{rosenblum}. \emph{Journaling} file systems apply the same idea to other file system types, logging only the intent to update metadata. Once the log is committed, the conventional structures can be updated asynchronously while still guaranteeing durability in the face of a system crash \cite{hagmann,sweeney,tweedie}. Breaking the chain of required metadata commits is also the objective of \emph{soft updates}, a technique that involves careful rearrangement of data in the buffer cache to permit delays and reordered writes \cite{ganger94}.

Studies comparing FFS with LFS \cite{seltzer95} and journaling with soft updates \cite{seltzer00} reveal a complicated picture. The requirement of cleaner process in LFS to reclaim space from the log interferes under some workloads, and updates to clustered file systems reduce the ordered-write problem enough to keep it competitive with small file updates. Will all of these systems, it is careful attention to the motion of the disk head that leads to good performance.

% TODO: This is a really weak section
What's the point?  How does this apply to Envoy?

In distributed systems or even local RAID arrays \cite{patterson}, higher-order effects can further distort the picture \cite{stein05}.

\subsection{Client-server file systems}

If local file systems can be characterized by how they manage disk head motion, distributed file systems are dominated by concerns of data placement and cache management. For a single host, cache management is easy. The OS has a monopoly on the disk and can reconcile any concurrent requests directly. Complications are mainly concerned with deciding when to commit writes to disk to ensure durability in the face of a system crash. Distributed systems must also consider consistency between caches on multiple machines. If a cache delivers an out-of-date version of a file, the application may be led to produce incorrect results.

The Sun Network File System (NFS) was the first widely-used file system for sharing files across hosts \cite{sandberg}. NFS serves many clients from a single access point (typically a server or a dedicated storage appliance \cite{hitz}), which hosts the persistent and canonical version of a file. Cache policy is unspecified, with no explicit support from the server. Clients generally cache reads and writes in memory and check with the server before relying on old cache entries (typically in the range of 10s of seconds). Thus an update made by one client is only detected by another when the first has sent the update to the server and the second has checked for an update. Clients can send a constant stream of \texttt{stat} requests to check for updates, but they cannot hasten a delayed update from another client, so they can never be assured of having the latest version of a file. An update to the protocol helped reduce traffic somewhat by performing implicit \texttt{stat} requests and including the results with common operations \cite{pawlowski,callaghan}, but the fundamental problem remained as clients still delay writes to the server. The latest update, NFSv4, includes features to improve cache management by \emph{delegating} complete control of individual files to clients and \emph{revoking} the delegation when other clients seek concurrent access.

Other client-server systems addressed the problem in different ways. The Andrew File System (AFS) \cite{satyanarayanan85,howard} uses the client's disk as a persistent cache with close-to-open semantics. In this scheme, the cache is validated at file open time, and changes forwarded to the server at file close time. To reduce validity checks at file open time, a client can be given a \emph{callback}, meaning that it can assume the file is current unless explicitly notified by the server. These changes improved scalability by enlarging the effective cache size on clients and reducing the load on the server, but they still left open the possibility of conflicting updates by multiple clients. Coda extended AFS to explore the problem of conflict resolution, opening up access to mobile users and allowing clients to continue operating when disconnected from the network \cite{satyanarayanan90,mummert}. DEcorum went the other way, extending AFS to strengthen cache consistency using tokens, as well as interopperate better with existing file systems and reduce recovery time after a crash \cite{kazar}.

The Sprite team observed that---despite the popularity of such systems---most applications do not explicitly account for inconsistencies introduced by the file system, so loosening consistency guarantees in favor of performance gains is a dangerous tradeoff. They sacrificed some performance for full cache consistency by disabling caching for files under contention \cite{baker91,nelson,welch}. Since concurrent access is relatively rare \cite{kistler}, this did not pose a significant problem for overall performance.

In all distributed file systems that permit sharing, there must either be a canonical version of the file (or block \cite{mcgregor}) or some way to reconcile conflicting updates \cite{kistler}. In the former case, some participant is usually nominated as the owner of a particular file through a lease \cite{gray89}, token passing \cite{burrows,mann,kazar}, or some other scheme. Any other host wanting access to the latest version must coordinate through the owner. Ownership may go to the host that provides storage, the one actively using the file, or a third manager host that connects the two \cite{blaze,keleher}. In Envoy, ownership goes to an active user, which then acts as a synchronous server to other users. Unlike Sprite, the principle user can continue to cache the file locally and share its cache with other concurrent users.

Another possibility is to disallow write sharing. The Cedar file system \cite{schroeder} makes all shared files immutable, and turns the problem into one of versioning \cite{gifford}. Venti takes this a step further by storing all file versions permanently and addressing them by a hash of their contents \cite{quinlan}. Since files are never changed or deleted, the store collects a complete history of all historical states of the file system. In workstation environments, it is possible for storage capacity to grow faster than storage is consumed, making this a feasible system, or past versions can be selectively removed as in the local file system Elephant \cite{santry}.

A less drastic approach is to permit snapshots \cite{hitz}, then mark historic versions as read-only with no requirement for cache coordination \cite{warfield}. Envoy employs this dichotomy between active and read-only file versions, implementing cache management only for mutable objects. It does not coalesce identical read-only objects like Venti or Farsite \cite{douceur02}. It could be extended to do so using a lazy, asynchronous process, but it reduces the need by promoting file system forking with copy-on-write as a management tool to avoid creating many of the duplicates in the first place.

Where snapshot systems typically use copy-on-write to transparently combine old data with new, some systems allow explicit stacking of file systems. Spring \cite{khalidi} allows file systems to be layered with optionally synchronized updates as a mechanism for extending functionality by layering in new features. Plan~9 \cite{pike90} allows any file system mount to be layered over another and their contents combined to give each user a custom view of local and remote file systems \cite{pike92}. The copy-on-write NFS server I developed for the XenoServers project \cite{kotsovinos04b} allowes layering instructions to be put in files, where the server immediately notices them and reconfigures the user's view of the file system. These systems can be used as a way to fork a file system, by sharing the common base image and capturing changes in a private layer. Over time this can lead to complex hierarchies of layers, and such systems rely on the semantics of their backing file systems. Envoy can be used in conjunction with a stacking layer, but it already provides explicit support for snapshots and file system forks.

\subsection{Serverless file systems}

While creative caching can alleviate the problem somewhat, all client-server architectures have inherent scaling problems. As a single point of contact for all clients, a server's load grows in proportion to the number of clients, and it also represents a single point of failure. In addition, the duties of a server tend to make it unsuitable for other uses, so such an architecture calls for a dedicated server. As workstations have grown in power, harnessing their excess capacity to cooperate on large problems has become increasingly attractive \cite{anderson95a}.

In xFS, the traditional roles of a server are split and distributed to the clients to yield a serverless architecture \cite{anderson95b}. Hosts can act as clients, managers that coordinate data placement, and/or storage servers that provide disk space, similar to the file system of the earlier LOCUS distributed operating system \cite{walker}. Cache coherency is achieved through an explicit consistency protocol where conflicting client requests are detected and managed.

In addition, the xFS team observed that modern networks make retrieval from a peer's cache faster than from a disk \cite{dahlin94b}, so sharing and coordinating cache across hosts can yield benefits over discrete local caches \cite{dahlin94a}. The resulting protocol was so complex that the team had to employ a formal protocol verifier to get a working implementation \cite{wang98}. While a valid way to manage complexity, reducing complexity through design may be preferable, especially in storage systems where correctness is paramount.

Farsite also targets a workstation environment, but assumes that participating machines are not trusted \cite{adya}. This requires encrypting data and using complex Byzantine agreement protocols instead of trusting hosts that have been assigned management roles. It also calls for a higher replication factor to guard against malicious attacks as well as hardware failures \cite{dahlin94a}, and makes cache sharing between hosts less practical. These restrictions are imposed by the environment, again highlighting the importance of matching storage design to the expected conditions.

The downside of having workstations double as servers is that the server function is not completely isolated from the other activities of the workstation. Runtime activity may be less predictable than on a dedicated server. Server load is normally determined by the aggregate activity of many clients instead of that of a single user, and a user may also switch a workstation off without notice. While a server can also fail unexpectedly, administration generally focuses on making this an infrequent event. The same trends that lead to excess capacity in workstations make dedicated servers cheap and powerful, without the additional complexity of a heterogeneous management environment. Despite numerous studies \cite{bolosky,douceur99,douceur01} showing feasibility and systems developed and tested \cite{adya,walker}, no serverless file system has seen widespread use for general-purpose computing.

While Envoy has similar goals for serving a location-independent file hierarchy to many untrusted clients, putting it in a trusted cluster environment changes the assumptions significantly. Hardware virtualization allows malicious clients to coexist on hardware with trusted server processes, and the complexity of Byzantine failure models can be avoided. Managed hardware also means that replication factors can be planned around hardware failure rates and load balancing without worrying about hosts switching off frequently.

\subsection{Wide-area file systems}

Carrying the idea of distributed file systems to the logical extreme leads to global file systems, running on hosts throughout the world and serving millions of clients located anywhere. Latency and available bandwidth become major concerns in this environment, and the inability to trust hosts forces widely-distributed file systems---as was true with Farsite in a smaller setting---to focus mainly on managing replicas for availability and reliability.

\subsubsection{Systems with servers}

Ficus \cite{guy} and Echo \cite{birrell93} link servers together to form a single, global file hierarchy, with transparent navigation between the discrete volumes that make up the system. In Echo, entire volumes are replicated in tightly-synchronized groups with one \emph{primary} server and one or more \emph{standby} servers. A token-passing scheme allows clients to cache files locally but still maintain global consistency \cite{mann}. Ficus relaxes the synchonization requirements in favor of optimistic concurrency, where conflicts must be resolved after being detected. Volume replicas are loosely synchronized, and may hold copies of only a subset of the files logically contained in the volume. Because of the loose synchronization, updates can be made to any file that has at least one replica available \cite{popek}. This permits continued operation in the face of network partitions or other failures, similar to the use of the persistent cache in Coda \cite{kistler}.

An early version of xFS \cite{wang93} also follows a two-tier model, where clusters of nearby hosts work together and share a single \emph{consistency server}. The consistency server acts as a proxy for the group when communicating in the wide area, and as a coordinator for operations within the cluster. Requests are served from the cache of a local host when possible, and forwarded to a remote cluster when necessary. Consistency is maintained through tokens that permit local caching for multiple readers or a single writer. To reduce the state that must be tracked, tokens cover entire groups of files.

JetFile \cite{gronvall} and Pangaea \cite{saito02a} rely on pervasive replication with little overall structure. JetFile  uses specialized servers for a few metadata functions, but most operations happen directly between clients. In both systems clients maintain replicas of all files they are interested in, and optimistic concurrency control permits disconnected operation. They differ in how updates are propogated. Pangaea maintains a replica graph for each file and pushes updates to other clients \cite{saito02b}, while JetFile makes extensive use of IP multicast to locate replicas and announce changes, and clients pull updates when required. When conflicting updates are detected in either system, the versions must be reconciled explicitly, generally using a last-writer-wins policy.

% Fluid Replication \cite{kim} 2002
% Segank: distributed mobile storage system \cite{sobti} 2004
% Optimizing for bandwidth consumpion \cite{muthitacharoen}
% Prospero \cite{neuman}

\subsubsection{Peer-to-peer systems}

Relying on trusted servers restricts the audience for a wide-area file system to large organizations and service providers who can maintain widely-dispursed networks. Peer-to-peer systems use resources volunteered by participants on large numbers of machines.

The simplest systems are read-only file distribution schemes. Bittorrent tracks all clients with a central manager, but data blocks are transferred mainly between clients \cite{cohen,pouwelse}, which request blocks they have not yet received from peers that have already downloaded them. Avalanche \cite{gkantsidis} uses network coding to decrease the incidence of ``rare'' blocks that make it difficult for clients to complete the last steps of a file download. Both systems feature capacity that grows with the number of users, without consuming excessive bandwidth at the server. Such systems are mainly useful for sharing the cost of distributing static content with those who benefit from it, and not for general-purpose storage needs.

Distributed hash tables (DHTs) such as CAN \cite{ratnasamy}, Pastry \cite{rowstron01b}, Chord \cite{stoica}, and Tapestry \cite{zhao01} provide distributed lookup services on peer-to-peer networks, and have been used as the basis for wide-area file systems. CFS \cite{dabek} and PAST \cite{rowstron01a} implement read-only systems using content-based addressing, similar to Venti \cite{quinlan}, but using an underlying DHT to locate object replicas. Other systems implement mutable file systems over similar substrates, storing data blocks, files, or content-based fragments \cite{rabin81} as immutable objects distributed across the network. Pasta \cite{moreton} stores metadata in special index blocks, each of which is associated with an asymmetric cryptographic key. The key is used to locate the index block (instead of using a hash of the contents as for normal data blocks) and to protect changes to it, allowing the index to change but retain a unique, static ID. Eliot \cite{stein02} stores metadata outside the immutable substrate, creating a seperate, writable system that references the read-only data indexed by the DHT. Ivy \cite{muthitacharoen02} stores all user changes to the user's log, which is then made available to others through the DHT, and each user consults as many logs as necessary to construct a coherent view of the file system.

Peer-to-peer file systems all suffer from the transience of users. Volunteered resources can be withdrawn without notice, and high levels of replication are required to ensure accessibility and availability \cite{blake,rabin89}. Latency is also high in the wide area, so further replication and caching is necessary to make performance acceptable. OceanStore \cite{kubiatowicz} was designed as a global network of smaller, highly-connected \emph{pools} that cooperate closely, with additional clustering of files that are regularly accessed together. The prototype, called Pond \cite{rhea}, uses Tapestry to organize virtual resources (data and managers), but it also forms localized \emph{rings} of participants to manage Byzantine agreement without the high latency typical of DHTs. The common element of these approaches is that they use tiered systems to reclaim some of the benefits of locality while still providing a global service.

Envoy has some structural parallels with these systems. While latency between machines in a cluster is much lower, Envoy still caches data on disk and in memory near the client. Locality is pursued at the machine level to aggregate the storage requirements of a set of virtual machines, with data ultimately replicated and spread throughout the cluster as storage objects.

The relationship of the present work to global storage systems is more complicated than a passing architectural resemblence, however. Envoy and the service clusters that host it are designed for commercial providers that locate computational resources near storage and fast network access. Instead of trying to hide the distance between users and data, service clusters share the goal of XenoServers \cite{reed} to move computation to a resource-rich environment. As a basic platform, service clusters backed by Envoy storage form the building blocks of global service networks, including storage services.

\subsection{Cluster storage systems}

The path from local file systems to wide-area file systems is generally progressive, with concerns about cache management, trust, latency, replica placement, and reliability growing at each step. Clusters have networks with low latency and high bandwidth, and large numbers of trusted, centrally-managed hosts. Their mixture of characteristics from the largest and smallest of systems reflects their popularity as replacements for traditional, monolithic supercomputer architectures.

With a large number of redundant components, clusters have the potential to be highly reliable (in the aggregate) with abundant bandwidth. One of the principal drivers of cluster storage systems is avoiding bottlenecks. While this is important in all systems, the vast aggregate disk and network bandwidth available to a large cluster makes it easy to overwhelm any single component \cite{hospodor}. In addition, the opportunity cost of wasted resources is potentially very high, so cluster systems must make good use of the combined storage capacity and I/O bandwidth available from the array of component machines.

\subsubsection{Storage layers}\label{sec:storage-layer-systems}

A common approach for cluster storage systems is to divide the problem into two distinct layers: one that manages the physical disks, balancing load and capacity and handling the addition and loss of disks, and a second layer that builds a file system above an abstract block- or object-level interface provided by the first.

Zebra \cite{hartman93} stripes data across multiple storage servers in a networked version of RAID \cite{patterson}, but uses a single file manager to coordinate metadata. Zebra is based on the Sprite LFS implementation, and clients cache data and metadata the same as they would in Sprite. The file manager assumes the role of a standard file server, but all log operations are striped and clients direct log operations directly to the storage servers. Swarm \cite{hartman99} removes the file manager to present a standalone layer that exports striped logs to clients, which can be used to implement local file systems or other high-level services.

Petal \cite{lee95,lee96} pools storage devices to export sparse virtual block devices to clients. All persistent state is maintained on the servers, with global state updated using a distributed agreement protocol that tolerates node failures. By caching a small amount of metadata, clients can direct most requests directly to the relevant storage server. Petal maintains a global map of servers for each virtual device, optionally using chained declustering for redundancy. Chained declustering \cite{hsiao} allows simple load redistribution when nodes fail and makes catastrophic failures more likely to damage a few virtual disks extensively than criple many virtual disks with small corruptions.

Frangipani \cite{thekkath} builds a file system over a Petal virtual disk. It takes advantage of the large, sparse address space to simplify data structures, with large regions reserved for inodes, allocation bitmaps, logs, small blocks, and large blocks. As with most layered systems, the physical layout is not determined by the virtual layout, so many of the layout strategies of local file systems are inapplicable \cite{stein05}. Instead, the layout of files is simple, with up to 16 4~KB blocks for small files spilling into one of $2^24$ large blocks for files up to 1~TB. File systems can be shared by multiple clients; a distributed lock manager coordinates caching and prevents corruption from concurrent access.

Object-based storage systems \cite{mesnier} provide an object-level interface to storage. Instead of exporting virtual disks to clients, they manage storage as collections of objects with attributes, which generally correspond directly to files and directories. Network-attached secure disks (NASD) drop storage servers and embed management functions directly in the storage device to save costs \cite{gibson97,gibson98a}. Metadata is controlled by a seperate server, They require a seperate metadata manager to from data access for shared storage, but cryptographic techniques make metadata operations largely asynchronous so most synchronous communications are directly between clients and the storage devices. 

FAB \cite{frolund,saito04} refines the ideas found in Petal and introduces the idea of \emph{storage bricks}---dedicated storage modules made from commodity components that can be combined to form a storage pool. FAB randomly assigns each data segment to a set of storage bricks and uses majority voting to manage replicas. This allows it to tolerate failures and add new bricks without pausing and temporarily ignore overloaded servers. It also supports fast snapshot operations without a centralized coordinator \cite{ji}.

The Self-* Storage project aims to extend brick-based storage to automate many aspects of configuration and management of storage in order to reduce administrative hassle and costs \cite{ganger03}. Like the AutoRAID storage device \cite{wilkes95}, the Ursa Minor prototype \cite{abd-el-malek} focuses on optimizing data layout and failure tolerance for different workloads and requirements. Using a unified infrastructure it supports multiple storage policies and can convert existing data while still operating normally.

Storage layers are complementary to the goals of Envoy. Indeed, the Envoy prototype's primitive storage layer is a stub intended to be replaced by a system much like those described here. Envoy assumes a reliable and performant storage layer, and builds a complete distributed file system optimized for large numbers of untrusted virtual machines above it.

\subsubsection{File systems}

Built on storage layers that can deliver parallel access to many disks, cluster file systems are left mainly with problem of managing metadata and maintaining cache consistency. Lustre \cite{lustre} follows model proposed by the NASD group \cite{gibson98a}, with clients contacting storage devices directly for object-level data access. Metadata is controlled by a centralized server with a failover replica.

The Google File System \cite{ghemawat} focuses on a specific workload and drops the POSIX file system interface. It, too, splits metadata management from data storage, but is further optimized for large files with sequential reads and append-only writes. Random reads and writes are supported, but the overall emphasis is on high throughput rather than low latency. In this environment, data caching has little value and metadata is minimized by managing files as sequences of large, 64~MB chunks. Chunks are stored as normal files using a Linux file system on commodity hardware.

Clusters are widely used as a replacement for supercomputers, and many cluster file systems are tuned to scientific-computing workloads. Like the Google environment, they often require high sustained throughput from large files. Unlike workstation environments where read-write sharing is rare, scientific computing makes frequent use of parallel processes writing to different parts of the same large file \cite{wang04}. GPFS \cite{schmuck} is optimized for large deployments where any centralized coordination point is unacceptable. Based on a block-level storage system, it supports a high degree of concurrency by replacing a centralized metadata manager with a distributed lock manager with byte-range locking, and using extensible hashing to support large directories that can be updated concurrently. In addition, it allows write sharing for non-conflicting metadata updates and pushes most of the communications burden for token revocation to the client triggering it, further reducing synchronous metadata operations in the lock manager.

Many commercial solutions use dedicated storage area networks (SAN) to connect hosts with storage devices over a high-speed switching fabric. As with most of the systems described here, SANs let clients connect directly to storage devices and rely on a seperate layer to mediate sharing and form a file system. Storage Tank \cite{menon} uses manually-configured partitions of the namespace to distribute control between metadata servers and relies on the block-based interface provided by SANs, but the overall structure is similar to systems like GPFS.

Ceph \cite{weil06} is also tuned to scientific cluster workloads, but is based on object storage. It, too, distributes metadata management to avoid bottlenecks, but instead of using distributed locks and having clients directly modify metadata structures, it builds a distributed version of the metadata manager common in other object-based systems. Object replicas are placed according to a special hash function, allowing clients to compute the location of an object instead of having to consult the metadata manager. Ceph handles the problem of heavy write sharing in scientific workloads by augmenting the POSIX interface to allow weakened consistency semantics. This works for scientific computing, where applications are generally custom-written anyway, but is less helpful in when clients use commodity tools. Like Envoy, Ceph divides management of the hierarchical namespace according to runtime activity, but it does so purely for load balancing within the distributed metadata service \cite{weil04}; no server benefits more than another from controlling a particular region of the namespace. Envoy, in contrast, delegates metadata management to the client dominating access to that part of the file system, making the placement of metadata management a matter of absolute performance as well as load distribution. It uses a time-based protocol to prioritize changes and promote stability (see \secref{sec:territory-management}).

While Envoy is also built for clusters, it is designed for many independent clients with more traditional workloads, rather than large, scientific systems bringing the power of thousands of machines to bear on a single dataset. Envoy creates a single hierarchical namespace, but it is arranged as an administrative tree with discrete, client-level file systems as leaves. Envoy is optimized to manage large numbers of these images, with full administration of a private image being managed by the machine using it, and control of shared images being distributed among the clients according to runtime demand. In this way, Envoy---with its model of a cluster being host to more clients than machines---is largely complementary to the systems described here, which attempt to make the cluster act like a single, large machine. Instead of centralizing metadata management and then introducing distribution schemes to overcome the limitations of centralization, Envoy partitions the namespace according to runtime demand and distributes metadata control to the clients, or more precisely to a secure virtual machine hosted on the same node as the client.

\subsection{Virtual machines}

Multi-Service Storage Architecture \cite{bacon}

CoWNFS \cite{kotsovinos04b}
GRID VMs \cite{zhao04} \cite{figueiredo01}
Parallax \cite{warfield}
Ventana \cite{pfaff}

% file systems for SANS \cite{burns}

\section{Summary}
