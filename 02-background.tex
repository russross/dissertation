\chapter{Background}

Storage systems have been studied and documented extensively in the systems literature. One could argue that everything worth doing has been done, or alternatively one could assert that the continuing research is evidence that everyone has failed so far, with the truth probably residing somewhere between these two extremes. Storage is an essential component of most systems, though, and the requirements placed on them are almost as various as the solutions that have been proposed.

In this chapter I survey the most important and widely known storage systems, with an emphasis on those that are relevent to service clusters and similar in design to Envoy. I describe some of the major design tradeoffs that emerge and identify the characteristics of service clusters that make existing systems unsuitable or less than ideal for this emerging environment as I justify the need for another storage system.

\section{Commodity computing}

\subsection{The GRID}
\cite{zhao}

\subsection{Embarassingly parallel}

\subsection{Service platforms}

Jini

\subsection{Network of workstations}
Networks of workstations (NOW)\cite{anderson95a}

\subsection{Clusters}
CARD for cluster monitoring\cite{anderson97}

Fast crash recovery \vs redundancy\cite{baker94}

Partition cloning \cite{rauch}

\subsubsection{TranSend}

TranSend was written for and the Inktomi search engine adapted to a layered cluster platform for scalable, homogeneous network services. Services composed of a few distinct types of actors could be managed by a common platform that provided monitoring and allocation of the component parts over the cluster. This type of platform exemplifies many of the advantages of clusters for network services, including load balancing, incremental scaling, fault tolerance, and high availability, but the implementation was limited to ``embarassingly parallel'' problems with trusted components and a custom state management system backed by a commercial SAN \cite{fox}. The authors were able to demonstrate that scalability was limited more by bandwidth into the cluster than by communication within it or coordination overhead.

\section{Virtualization}

\subsection{Virtual machine monitors}
\subsubsection{IBM System/370}
\subsubsection{Disco}
\subsubsection{Denali}
\subsubsection{Xen}
\cite{barham}
Live migration \cite{clark} \cite{sapuntzakis}
\subsubsection{Terra}
\cite{garfinkel}
\subsubsection{VMWare}
\subsubsection{vMatrix}
\subsubsection{IBM Managed Hosting}

\subsection{XenoServers}
\cite{kotsovinos}

Ventana \cite{pfaff}

\section{File systems}

\subsection{Early systems}
\subsubsection{Multics}
\cite{corbato}
\subsubsection{The Tandem NonStop kernel}
\cite{bartlett}
\subsubsection{Grapevine}
\cite{birrell82}
\subsubsection{Worm}
\cite{shoch}

\subsection{Local file systems}
\subsubsection{Berkeley FFS}
\subsubsection{Elephant}
\cite{santry}

\subsection{Network file systems}
\subsubsection{Cedar}
\cite{gifford,hagmann}
\subsubsection{NFS}
\cite{pawlowski,callaghan}
\subsubsection{NFSv4}
\subsubsection{Sprite}
\cite{baker91,nelson}
\subsubsection{AFS}
\cite{howard}
\subsubsection{Coda}
\cite{kistler,satyanarayanan}
\subsubsection{Alpine}
\cite{brown}
\subsubsection{Spring}
\cite{khalidi}
\subsubsection{Harp}
\cite{liskov}
\subsubsection{Plan 9}
Plan 9 \cite{pike90} name spaces \cite{pike92} in Linux with v9fs \cite{hensbergen}
\subsubsection{Caching}
\cite{dahlin94a}\cite{chaiken}\cite{blaze}\cite{dahlin94b}\cite{keleher}\cite{muthitacharoen}
\subsubsection{Disk layout}
Zebra \cite{hartman} is network RAID \cite{patterson}

AutoRAID \cite{wilkes}

Block-based sharing \cite{mcgregor}

\subsection{Serverless file systems}
\cite{douceur01}\cite{douceur02}
\subsubsection{xFS}
xFS\cite{wang93,anderson95b,wang98}
\subsubsection{Farsite}
\cite{bolosky}

\subsection{Archival systems}

\subsubsection{Venti}
\cite{quinlan}

\subsection{Wide-area file systems}
Prospero \cite{neuman}

Oceanstore and Pond \cite{rhea}

\subsubsection{Echo}

The Echo distributed file system \cite{birrell93} was designed to provide a single, global namespace that was accessible anywhere and perfectly coherent. It uses write-behind caching for files and directories with ordering constraints to preserve consistency even in the presence of crashes \cite{mann}, and employs a token-passing scheme to manage cache coherency. The top levels of the namespace are hosted on the DNS, with a link to a specific server. From there, servers host branches of the tree with further links to additional servers. Replication is done at the server level with primary and secondary servers that are synchronized. It died when the OS that hosted it wasn't ported to newer hardware (another reason to use commodity systems).

\subsection{Peer-to-peer file systems}
\subsubsection{Pasta}
\cite{moreton}
\subsubsection{Pangaea}
\cite{saito02}
\cite{stein}

\subsection{Mobile computing support}
\cite{kim}\cite{mummert}\cite{sobti}

\subsection{Clusters}

\subsubsection{Petal}
\cite{lee95,lee96}
\subsubsection{Frangipani}
\cite{thekkath}
\subsubsection{Ursa Minor}
\cite{abd-el-malek}
\subsubsection{Google file system}
\cite{ghemawat}
\subsubsection{Lustre}
\cite{lustre}
\subsubsection{Parallax}
\cite{warfield}
\subsubsection{FAB}
\cite{frolund,saito04}

\subsection{Storage area networks}
\cite{burns}

Network-attached secure disks (NASD) \cite{gibson}

Network-attached storage (NetAPP) \cite{hitz}

\section{Summary}
