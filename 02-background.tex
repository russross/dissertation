\chapter{Background}\label{cha:background}

This dissertation builds on the rich body of research and systems that have preceded it. In this chapter I present relevant background material to establish the context of the current work. First comes a discussion of commoditisation within the computer industry and how it relates to the proposal in \charef{cha:motivation} for a commodity computation platform. Next, the current state of machine virtualization---an important component of the proposed platform---is presented with an emphasis on recent systems for the x86 architecture. The chapter then turns to influential storage systems and other work that relates to the Envoy file system described in later chapters.

\section{Commodity computing}

Commoditisation relates to computing in two distinct ways, both of which are relevant here. The first is the general consolidation of PC hardware into a series of commodity parts, which are cheap enough and powerful enough to take over many applications that were the domain of specialised ``big-iron'' hardware in the past. The second is the push to offer computation as a service instead of as a product, where billing is based on work done rather than on hardware and software delivered. This dissertation proposes using clusters of commodity hardware as a platform for offering commodity computation services.

\subsection{Commodity big iron}

The scale of the PC market ensures that the best the semiconductor market has to offer is available for PCs. The difference between cheap commodity hardware and expensive mainframes and supercomputers is in scalability, reliability, and manageability. High-end equipment is designed to offer high bandwidth on buses and I/O channels and to scale through highly-parallel configurations. Redundant components mask some classes of faults, and others can be isolated and repaired without interrupting the operation of unaffected components. While these features could be offered in commodity machines, they add too much cost to be worthwhile in environments where service interruption is more annoying than expensive.

\subsubsection{Networks of workstations}

The proposal for Networks of Workstations (NOW) was an early argument for using networks of smaller computers for large problems instead of seeking parallelism through specialised hardware \cite{anderson95a}. While individual components are less performant, the aggregate capacity of a large collection of machines is immense. Commodity disks may be slow, but accessing them in parallel yields high bandwidth as well as high capacity. Reliability and fault tolerance can be designed as a property of the combined system, rather than being engineered into individual hardware components. This has the advantage of transferring the expense from a per-unit manufacturing overhead to a one-time cost in the software design.

PCs have encroached on the workstation market and the two terms are largely synonymous now. The same trends that prevailed in the mid~90s for workstations continue today, however. Switched networks lead to aggregate bandwidth that scales with the number of hosts. Single-threaded performance on cheap PCs is a significant fraction of that on the fastest server hardware. Disks continue to grow in capacity, but their performance improves at a much slower rate. Performance is best scaled through parallelism, and a collection of many machines that all have cheap disks has the potential to outperform even the fastest single controller or collection of high-performance disks attached to a single machine.

\subsubsection{Clusters}

While workstations in an organisation can be profitably pooled for large jobs, clusters are specially built for that purpose and eliminate the workstation role from a node's list of duties. The basic principles behind clusters were laid down in the 1960s \cite{amdahl}, but the power and affordability of recent commodity hardware has brought clusters to renewed prominence. The same hardware used in high-street PCs powers some of the most powerful computing sites in the world \cite{top500}.

Dedicated clusters have a few advantages over more loosely-organised structures. Nodes are typically housed in a controlled environment with redundancy built into power supplies, network links, and cooling equipment. While many clusters grow incrementally, they are still relatively homogeneous, being built from batches of identical hardware. They are also professionally managed and monitored with central control over all resources, unlike looser amalgamations of machines.

As parallel computing vehicles, clusters are mainly used for large ``embarrassingly parallel''\footnote{See \url{http://en.wikipedia.org/wiki/Embarrassingly_parallel}} problems that can be neatly decomposed into discrete tasks. Google has been prominent among commercial users of commodity clusters, and their search engine architecture is an example of a highly-parallel task. The search index can be cloned to serve many concurrent transactions, and partitioned to split processing for a single query over multiple nodes \cite{barroso03}. Scientific workloads, simulations, and graphics rendering farms are all characterised by repetitive computations over huge data sets, and are similarly well-suited to cluster implementation. Beowulf is a popular system specifically designed for addressing these kinds of tasks on clusters of commodity hardware running Linux or other Unix-like operating systems \cite{ridge}.

These problems are generally large enough to warrant custom software solutions as well as dedicated hardware resources. Management software for clusters usually assumes that the application is trusted and cooperative. Systems like TranSend \cite{fox} and CARD \cite{anderson97} require application software to be divided into discrete application units that the monitoring software can direct. Migrating tasks from node to node and compensating for failed nodes requires cooperation from the application, or interaction with a transaction interface that exposes application semantics. Beowulf \cite{ridge} is a lower-level toolkit, providing libraries of useful tools for building a custom cluster application.

For some classes of problems, clusters dominate as the architecture of choice, with clusters of commodity hardware increasingly winning over clusters of specialised hardware. This dissertation does not seek to supplant them for inherently parallel tasks, but rather to extend their advantages into other problem domains. While clusters are a sound choice for problems requiring the dedicated effort of thousands of machines, they can be an equally viable platform for problems too small to require the undivided attention of even a single node. Clusters are normally built and located for a specific user and a specific task, but they can also be built where conditions are best---perhaps near network interconnect points or where cheap resources abound or a favourable regulatory environment exists; strategic choices can give vendors a competitive edge. The advantages of scale can be brought to bear on small and medium-sized problems, not just on the largest of tasks.

\subsection{Computation for hire}

Computer hardware is cheap enough that most organisations can afford to buy systems with sufficient power for their needs. However, because of the complexity of managing computer systems and the ongoing operating and maintenance costs, this is not always the best way to proceed. Numerous avenues exist for clients wanting to hire computation services instead of owning and managing their own hardware and software.

\subsubsection{Replacing the machine room}

The primary motivation for outsourcing computation as considered here is to reduce or eliminate the role of the machine room. Dedicated server hardware comes with expenses that may outweigh the benefits of local ownership and control.

The most obvious costs are direct: physical space required, electricity consumed, and heat produced. Commodity hardware is cheap enough that these runtime costs may exceed the capital cost of the hardware over its service lifetime \cite{barroso05}. Hardware must either be located at a company site, where appropriate facilities must be installed and internet connectivity hired, or at a dedicated hosting facility, where managing it is more complicated and may require sending staff to a remote location.

Maintaining servers also requires qualified staff, another significant expense. The skills required to manage computer systems are unrelated to the core mission of many organisations, forcing them to develop a sizable technical staff to support operations that have little to do with technology. Running a parallel group outside an organisation's core expertise may lead to additional inefficiencies.

These costs are even less palatable when the computation facilities are only needed intermittently. Unpredictable demand may force planning based on peak requirements, but many costs are fixed and do not drop during periods of low demand. When asked about the expenses of their large computing infrastructure, Jeff Bezos of Amazon claimed that cost from lack of utilisation dominated the costs of power, servers, and people \cite{bezos}.

\subsubsection{Hosted services}

Some services are generic enough to be hosted remotely by specialised providers, who make money by offering the same inexpensive service to many clients. Basic applications like email, payroll management, web hosting, and other common services have already attracted commercial interest. Hosting them locally brings all the negative aspects of managing an on-site machine room, while adding little or no value over using a remote provider.

Hosted software eliminates the need for the client to buy or manage dedicated hardware, and the application is managed by specialists who can amortise fixed costs over a large customer base. Upgrades do not require work from the client, making it practical to keep all customers synchronised and further reduce support costs. The unpredictable demand of individual clients can become predictable in the aggregate, allowing more efficient hardware use.

Similar arguments apply to storage outsourcing \cite{ng}, for those wishing to hire managed storage while retaining local control of applications. Multi-site backups, capacity planning, and other domain-specific concerns can be turned over to a specialist, leaving local staff to focus on the problems unique to the organisation.

Hosted services fit well with the present proposal, which takes the concept a step further by splitting management of hardware from management of software. In the service cluster model advocated here, service providers can focus entirely on their applications, leaving hardware management to a dedicated hosting service. Alternatively, clients can hire software management services from one vendor and hardware hosting directly from another, isolating the specific value offered by a software vendor from the basic operating costs that all software applications require.

\subsubsection{Utility computing}

Application hosting is most practical for widely-used services, where few differences exist between clients. The proposal in this dissertation is a form of utility computing, where hardware resources are made available for custom applications at a cost related to resource consumption. \emph{Utility computing} refers to a range of computing models, with the common characteristic of on-demand resource provision. Hosted services as described above, and grid computing as described below, fall under the umbrella of utility computing.

Existing work focuses mainly on large applications where abundant resources are needed, but not necessarily over a long enough period to justify building a suitable hardware solution. An example is rendering feature-length animated films, which requires vast computing power and complex interactions, but only until the project is finished. Vendors with sufficient resources can manage them like batch processors on early mainframes, assigning hardware to a series of applications that have been suitably prepared for the environment \cite{wilkes04}. HP's Utility Data Center (UDC) \cite{kallahalla} uses virtualization to manage services, much like the service clusters described in \charef{cha:motivation}, but focuses on large projects that can be individually adapted to the UDC environment.

The Collective project also uses virtualization to isolate services, and proposes turning the desktop PCs of an organisation into a pool of utility computing hosts to simplify administration \cite{sapuntzakis03}. To end users, interactive applications and services appear as \emph{virtual appliances} that can be instantiated on a local PC. This allows specialised staff to manage desktop software as well as machine-room applications. Desktop PCs act somewhat like terminals for managed software, but terminals that also host computation for their own clients. In some sense, this is the reverse of other utility computing environments; it delivers managed software services to local hardware instead of offering hosted hardware to run client applications.

\subsubsection{Grid computing}

Grid computing \cite{foster,sungrid} seeks to combine resources in the wide area to provide on-demand computation. Utility computing gets its name from basic utilities like power, water, and sewer service, offering a common service to many clients who pay according to what they use. Grid computing styles itself after power grids, which provide a meshed infrastructure to match power producers with consumers across the boundaries of a single utility.

Pooling the resources of many organisations has the potential to put vast resources at the disposal of clients, but it also introduces new problems. Unlike the cluster environments typical of utility computing, grid applications must communicate over the wide area with its low bandwidth and high latency. This makes the grid model most suitable for highly parallel CPU-intensive jobs, which can be parcelled into discrete units and distributed to remote nodes. Moving from a single, trusted management environment makes security considerations more complex as well.

Grid computing requires special middleware, though using virtual machines to support more general-purpose clients has been proposed \cite{figueiredo03,zhao04}. Even then, poor connectivity makes it slow and expensive to transfer large data sets or share data extensively between nodes. The distribution model of grid computing makes it poorly suited to interactive applications or those that require close cooperation between nodes.

Existing utility computing solutions do little to address the needs of an organisation with a relatively small computing infrastructure hoping to stay out of the hardware management business. A few applications can be outsourced to a service provider, and large jobs can be run on hired hardware, but these systems are only applicable to a small subset of the applications running in a typical organisation's machine room. To achieve the scale necessary to make an offering profitable, vendors have focused on hosting widely-used applications and projects that are already large. This differs from the current proposal, which aims to make hosted computation a commodity service practical for a wider range of client applications.

\section{Virtualization}

Resource virtualization makes it possible to put familiar tools and interfaces in new environments without substantial modifications. Virtual machine abstractions allow standard operating systems to share hardware even when they are designed to assume exclusive control. Virtual disk abstractions provide a block interface like that of a standard disk, but without tying it directly to a single piece of hardware. Virtualization is used for a variety of reasons, but the relevant purpose here is for resource management and sharing.

\subsection{Virtual machine monitors}

IBM VM/370 \cite{gum} first virtualized hardware to support legacy code without modification. Modern commercial systems like VMWare \cite{vmware} and Microsoft Virtual Server \cite{microsoftvm} are often used for similar purposes, allowing standard operating systems to run unmodified as guests in new environments. This is useful for testing labs, which can test applications against a range of software configurations on limited hardware, and for end users who need access to applications from multiple software platforms. While this allows access to a wide range of guest software, it comes at a performance cost on some hardware---including the standard x86 platform---which was not designed with virtualization in mind.

Virtual machine monitors (VMM) work by imposing a thin software layer between the operating system and the bare hardware. Unlike emulators or simulators, VMMs allow code to execute directly on the hardware whenever possible. Certain operations break the illusion of isolation, however, particularly those involving I/O, memory protection, and interrupt management. The VMM must intercept these operations and simulate the desired effects on the virtual machine. If the hardware does not allow the VMM to directly intercept such operations, it must do so using software means, such as code rewriting or emulation. The overhead of such virtualization techniques is highest when I/O operations are frequency, as is common in server applications.

The Xen hypervisor uses \emph{paravirtualization}, where the operating system is explicitly ported to the hypervisor environment. By eliminating expensive virtualization techniques for I/O operations, paravirtualization leads to low overhead even for network and database applications with heavy I/O components \cite{barham}. This does not affect userspace applications, whose I/O operations are already virtualized by the operating system, so unmodified binaries can still be used with the modified operating system. The down side is that paravirtualization does not provide support for systems that are not modified to support it, including legacy systems. The combination of high performance and support for modern commodity systems makes it suitable as a host for a managed platform like the one proposed here. Recent updates to the x86 platform support full hardware virtualization \cite{adams}, allowing unported systems to run as well, although explicit support for the paravirtualized environment is still beneficial in reducing overhead.

Virtualization allows a commodity computation platform to isolate hosted services from each other and allow them to be managed without explicit cooperation at the application level. Multiple guest operating systems can run on the same hardware, and virtual machines can be migrated transparently to balance load, collocate related services, or free a machine for servicing \cite{clark}. Virtual machines can even be frozen and migrated to remote sites \cite{sapuntzakis02}, a deployment strategy used by vMatrix \cite{awadallah} to migrate and replicate specialised internet services.

The IBM zSeries \cite{ibmzseries} platform provides a managed hosting environment for paravirtualized Linux, but it does so on specialised mainframe hardware and ties clients to a single vendor solution. The availability of fast, secure virtualization for cheap commodity hardware opens up the possibility of applying these techniques to create an open commodity market for computation.

\subsection{XenoServers}

A public computing platform requires more than just hosting technology. The XenoServers project \cite{reed,kotsovinos04a} explores the infrastructure required to host network services on a public computing platform. The Xen hypervisor started as part of the XenoServers project, which motivated its emphasis on performance over compatibility with legacy code. Application binaries can run without modification in a paravirtualized system, so the modifications Xen requires only affect the operating system.

The XenoServers project also addresses the problem of helping clients find a suitable host \cite{spence}. Network services may have specific connectivity requirements, especially when being deployed for use by a small group or an individual. For example, a game server connecting a group of players requires low latency to a specific set of clients to support real-time interaction. To support a large number of hosting providers, XenoServers also defines the infrastructure for billing through a trusted third party.

This dissertation is not directly linked to the XenoServers project, but it is related. XenoServers provides much of the general framework for public computing, but it does not specify the architecture for hosts beyond the use of the Xen hypervisor. In this work I propose using clusters of commodity hardware as the basis for public computing, and I offer a storage system suitable for that environment. The Envoy file system proposed here also supports a deployment strategy similar to the one we described as part of the XenoServers project \cite{kotsovinos04b}.

\section{Storage systems}

Storage is a fundamental component of all general-purpose computer systems. A combination of high storage density, random access, and low cost has made magnetic storage on rotating platters the dominant medium for durable storage. The time to access a random byte of data from a disk is typically measured in milliseconds, while for DRAM this time drops roughly six orders of magnitude to a nanosecond scale, and that gap is continuing to grow.

Because disks are so slow, storage systems are designed around the goal of accessing the disk as little as possible, and favouring sequential access to avoid costly seek delays. Effective use of cache is vital to this goal, and specialised storage systems tuned to specific system architectures and expected workloads are worthwhile because of the potential speed gains over more general systems.

Storage systems have been studied extensively, and one purpose of this section is to survey related work that has influenced this dissertation. A second purpose relates to the thesis of this work, which argues that an environment can only succeed with an appropriate storage architecture. The works discussed here are presented in the context of the environments they support, in part to establish a pattern of symbiosis between storage architectures and computation environments. Envoy, the file system introduced in this dissertation, is discussed briefly in this section to relate it to prior work, but details of its design are reserved for later chapters.

\subsection{Local file systems}

Disks are mechanical devices and their performance is limited primarily by the need to physically position the disk head over the correct part of the platter to access data. File systems designed for local disks achieve performance by minimising the physical movement required. Disk access can be reduced through caching, and hardware latency can be minimised by arranging data on the disk to minimise movement. Correctness and reliability tend to trump performance as design considerations, however, and the wide variety of possible workloads makes it difficult to pick clear winners from competing designs.

The Berkeley Fast File System (FFS) was the first to optimise data layout for performance by clustering related information. Block metadata is distributed across the disk to be near the file data it describes, file metadata is grouped for files in the same directory, and blocks in the same file are grouped whenever possible \cite{mckusick}. \emph{Clustering}, as this design is called, exploits concepts that extend beyond local file systems: the same expectations of correlated access can be exploited to reduce latency when accessing data across the network \cite{amer}.

File system tracing reveals how files are accessed in real systems \cite{ousterhout}. Most files accessed are small, but size distributions are skewed enough that most data transferred is from large files. Writes are less common than reads, and most files are short-lived. Files access is typically sequential, and most files are read from or written to in their entirety. These trends have proved resilient over time, though the scale has increased and the largest files in typical systems have grown much larger \cite{ruemmler,gibson98b,roselli}.

As cache sizes grow, more requests can be satisfied without consulting the disk, and designs can assume that many read requests can be served from memory. All writes whose effects are not quickly undone have to go to disk eventually, so despite being less frequent overall, writes requests can dominate the mix of operations that penetrates the cache and reaches the disk. Furthermore, updates often involve metadata changes as well, potentially requiring multiple costly disk seeks even for small updates.

\emph{Log-structured file systems} (LFS) address this problem by borrowing from database design and making the entire file system an append-only log. Writes are gathered and written as sequential chunks on disk, with relevant metadata rewritten instead of updating existing structures directly \cite{rosenblum}. \emph{Journaling} file systems apply the same idea to other file system types, logging only the intent to update metadata. Once the log is committed, the conventional structures can be updated asynchronously while still guaranteeing durability in the face of a system crash \cite{hagmann,sweeney,tweedie}. Breaking the chain of synchronous metadata commits is also the objective of \emph{soft updates}, a technique that involves careful rearrangement of data in the buffer cache to permit delays and reordered writes \cite{ganger94}.

Studies comparing FFS with LFS \cite{seltzer95} and journaling with soft updates \cite{seltzer00} reveal a complicated picture. Transaction workloads force frequent synchronous writes to support their own semantics, negating the benefits of aggregated writes, and updates to clustered file systems reduce the ordered-write problem enough to keep it competitive with LFS for small file updates. Very different strategies can lead to comparable results, but for all local file systems it is careful attention to the motion of the disk head that leads to good performance.

\subsection{Client-server file systems}

If local file systems can be characterised by how they manage disk head motion, distributed file systems are dominated by concerns of data placement and cache management. For a single host, cache management is easy. The OS has a monopoly on the disk and can reconcile any concurrent requests. Complications are mainly concerned with deciding when to commit writes to disk to ensure durability in the face of a system crash. Distributed systems must also consider consistency between caches on multiple machines. If a cache delivers an out-of-date version of a file, the application may be led to produce incorrect results.

The Sun Network File System (NFS) was the first widely used file system for sharing files across hosts \cite{sandberg}. NFS serves many clients from a single access point (typically a server or a dedicated storage appliance \cite{hitz}), which hosts the persistent and canonical version of a file. Cache policy is unspecified, with no explicit support from the server. Clients generally cache reads and writes in memory and check with the server before relying on old cache entries (typically in the range of 10s of seconds). Thus an update made by one client is only detected by another when the first has sent the update to the server and the second has checked for an update. Clients can send a constant stream of \texttt{stat} requests to check for updates, but they cannot hasten a delayed update from another client, so they can never be assured of having the latest version of a file. An update to the protocol helped reduce traffic somewhat by performing implicit \texttt{stat} requests and including the results with common operations \cite{pawlowski,callaghan}, but the fundamental problem remains as clients still delay writes to the server. The latest update, NFSv4, includes features to improve cache management for uncontended data by \emph{delegating} complete control of individual files to clients and \emph{revoking} the delegation when other clients seek concurrent access, at which point it reverts to the consistency semantics of earlier versions \cite{shepler}.

Other client-server systems address the problem in different ways. The Andrew File System (AFS) \cite{satyanarayanan85,howard} uses the client's disk as a persistent cache with \emph{close-to-open} semantics. In this scheme, the cache is validated at file open time, and changes forwarded to the server at file close time. To reduce validity checks at file open time, a client can be given a \emph{callback}, meaning that it can assume the file is current unless explicitly notified by the server. These changes improve scalability by enlarging the effective cache size on clients and reducing the load on the server, but they still leave open the possibility of conflicting updates by multiple clients. Coda extends AFS to explore the problem of conflict resolution, opening up access to mobile users and allowing clients to continue operating when disconnected from the network \cite{satyanarayanan90,mummert}. DEcorum goes the other way, extending AFS to strengthen cache consistency using \emph{token passing}---a scheme where each file has a single logical token that a client must obtain before it can write to the file---as well as to interoperate better with existing file systems and reduce recovery time after a crash \cite{kazar}.

The Sprite team observed that---despite the popularity of NFS---few applications explicitly address inconsistencies introduced by the file system, so loosening consistency guarantees in favour of performance gains is a dangerous tradeoff. They sacrificed some performance for full cache consistency by disabling caching for files under contention \cite{baker91,nelson,welch}. Since concurrent access is relatively rare \cite{kistler}, this does not pose a significant problem for overall performance.

In all distributed file systems that permit sharing, there must either be a canonical version of the file (or block \cite{mcgregor}) or some way to reconcile conflicting updates \cite{kistler}. In the former case, some participant is usually nominated as the owner of a particular file through a lease \cite{gray89}, token passing \cite{burrows,mann,kazar}, or some other scheme. Any other host wanting access to the latest version must coordinate through the owner. Ownership may go to the host that provides storage, the one actively using the file, or a management host that connects the two \cite{blaze,keleher}. In Envoy, ownership goes to an active user, which then acts as a synchronous server to other users. Unlike Sprite, the principal user can continue to cache the file locally and share its cache with other concurrent users.

Another possibility is to disallow write sharing. The Cedar file system \cite{schroeder} makes all shared files immutable, and turns the problem into one of versioning \cite{gifford}. Venti takes this a step further by storing all file versions permanently and addressing them by a hash of their contents \cite{quinlan}. Since files are never changed or deleted, the store collects a complete history of all historical states of the file system. In workstation environments, it is possible for storage capacity to grow faster than storage is consumed, making this a feasible system, or past versions can be selectively removed as in the local file system Elephant \cite{santry}.

A less drastic approach is to permit snapshots \cite{hitz}, then mark historical versions as read-only with no requirement for cache coordination \cite{warfield}. Envoy employs this dichotomy between active and read-only file versions, implementing cache management only for mutable objects. It does not coalesce identical read-only objects like Venti or Farsite \cite{douceur02}. It could be extended to do so using a lazy, asynchronous process, but it reduces the need by promoting file system forking with copy-on-write as a management tool to avoid creating many of the duplicates in the first place.

Where snapshot systems typically use copy-on-write to transparently combine old data with new, some systems allow explicit stacking of file systems. Spring \cite{khalidi} allows file systems to be layered with optionally synchronised updates as a mechanism for extending functionality by layering in new features. Plan~9 \cite{pike90} allows any file system mount to be layered over another and their contents combined to give each user a custom view of local and remote file systems \cite{pike92}. The copy-on-write NFS server I developed for the XenoServers project \cite{kotsovinos04b} allows layering instructions to be put in a file in any directory, directing the server to immediately reconfigure the user's view of the file system. These systems can be used as a way to fork a file system, by sharing the common base image and capturing changes in a private layer. Over time this can lead to complex hierarchies of layers, and such systems rely on the semantics of their backing file systems. Envoy can be used in conjunction with a stacking layer, but it already provides explicit support for snapshots and file system forks.

\subsection{Serverless file systems}

While creative caching can alleviate the problem somewhat, all client-server architectures have inherent scaling problems. As a single point of contact for all clients, a server is subject to load that grows in proportion to the number of clients, and it also represents a single point of failure. In addition, the duties of a server tend to make it unsuitable for other uses, so such an architecture calls for a dedicated server. As workstations have grown in power, harnessing their excess capacity to cooperate on large problems has become increasingly attractive \cite{anderson95a}.

In xFS, the traditional roles of a server are split and distributed to the clients to yield a serverless architecture \cite{anderson95b}. Hosts can act as clients, managers that coordinate data placement, and/or storage servers that provide disk space, similar to the file system of the earlier LOCUS distributed operating system \cite{walker}. Cache coherency is achieved through an explicit consistency protocol where conflicting client requests are detected and managed.

In addition, the xFS team observed that modern networks make retrieval from a peer's cache faster than from a disk \cite{dahlin94b}, so sharing and coordinating cache across hosts can yield benefits over independent local caches \cite{dahlin94a}. The resulting protocol was so complex that the team had to employ a formal protocol verifier to get a working implementation \cite{wang98}. While a valid way to manage complexity, reducing complexity through design may be preferable, especially in storage systems where correctness is paramount.

Farsite also targets a workstation environment, but assumes that participating machines are not trusted \cite{adya}. This requires encrypting data and using complex Byzantine agreement protocols instead of trusting hosts that have been assigned management roles. It also calls for a higher replication factor to guard against malicious attacks as well as hardware failures \cite{dahlin94a}, and makes cache sharing between hosts less practical. These restrictions are imposed by the environment, again highlighting the importance of matching storage design to expected conditions.

The downside of having workstations double as servers is that the server function is not completely isolated from the other activities of the workstation. Server load is normally determined by the aggregate activity of many clients instead of that of a single unpredictable user, and a user may also switch a workstation off without notice. While a server can also fail unexpectedly, careful administration generally makes this an infrequent event. Extra redundancy is necessary to make files \emph{available}, even when they are still reliably stored on a workstation that has been powered down.

The same trends that lead to excess capacity in workstations make dedicated servers cheap and powerful, without the additional complexity of a heterogeneous management environment. Despite numerous studies showing feasibility \cite{bolosky,douceur99,douceur01} and systems developed and tested \cite{adya,walker}, no serverless file system has seen widespread use for general-purpose computing.

While Envoy has similar goals for serving a location-independent file hierarchy to many untrusted clients, putting it in a trusted cluster environment changes the assumptions significantly. Hardware virtualization allows malicious clients to coexist on hardware with trusted server processes, and the complexity of Byzantine failure models can be avoided. Managed hardware also means that replication factors can be planned around hardware failure rates and load balancing without worrying about hosts being switched off by desktop users.

\subsection{Wide-area file systems}

Carrying the idea of distributed file systems to the logical extreme leads to global file systems, running on hosts throughout the world and serving millions of clients located anywhere. Latency and available bandwidth become major concerns in this environment, and the inability to trust hosts forces widely-distributed file systems---as was true with Farsite in a smaller setting---to focus mainly on managing replicas for availability and reliability.

\subsubsection{Systems with servers}

Ficus \cite{guy} and Echo \cite{birrell93} link servers together to form a single, global file hierarchy, with transparent navigation between the discrete volumes that make up the system. In Echo, entire volumes are replicated in tightly-synchronised groups with one \emph{primary} server and one or more \emph{standby} servers. A token-passing scheme allows clients to cache files locally but still maintain global consistency \cite{mann}. Ficus relaxes the synchronisation requirements in favour of optimistic concurrency, where conflicts must be resolved after being detected. Volume replicas are loosely synchronised, and each may hold copies of only a subset of the files logically contained in the volume. Updates can be made to any file that has at least one replica available \cite{popek}, permitting continued operation in the face of network partitions or other failures, similar to Coda \cite{kistler}.

An early version of xFS \cite{wang93} also follows a two-tier model, where loose clusters of nearby hosts work together and share a single \emph{consistency server}. The consistency server acts as a proxy for the group when communicating in the wide area, and as a coordinator for operations within the cluster. Requests are served from the cache of a local host when possible, and forwarded to a remote cluster when necessary. Consistency is maintained through tokens that permit local caching for multiple readers or a single writer. To reduce the state that must be tracked, tokens cover entire groups of files.

JetFile \cite{gronvall} and Pangaea \cite{saito02a} rely on pervasive replication with little overall structure. JetFile uses specialised servers for a few metadata functions, but most operations happen directly between clients. In both systems clients maintain replicas of all files they are interested in, and optimistic concurrency control permits disconnected operation. They differ in how updates are propagated. Pangaea maintains a replica graph for each file and pushes updates to other clients \cite{saito02b}, while JetFile makes extensive use of IP multicast to locate replicas and announce changes, and clients pull updates when required. When conflicting updates are detected in either system, the versions must be reconciled explicitly, generally using a last-writer-wins policy.

\subsubsection{Peer-to-peer systems}

Relying on trusted servers restricts the audience for a wide-area file system to large organisations and service providers who can maintain widely-dispersed networks. Peer-to-peer systems use resources volunteered by participants on large numbers of machines.

The simplest systems are read-only file distribution schemes. Bittorrent tracks all clients with a central manager, but data blocks are transferred mainly between clients \cite{cohen,pouwelse}, which request blocks they have not yet received from peers that have already downloaded them. Avalanche \cite{gkantsidis} uses network coding to decrease the incidence of ``rare'' blocks that make it difficult for clients to complete the last steps of a file download. Both systems feature capacity that grows with the number of users, without consuming excessive bandwidth at the server. Such systems are mainly useful for sharing the cost of distributing static content with those who benefit from it, and not for general-purpose storage needs.

Wide-area file systems can also be built using \emph{distributed hash tables} (DHT) such as CAN \cite{ratnasamy}, Pastry \cite{rowstron01b}, Chord \cite{stoica}, and Tapestry \cite{zhao01}, which provide distributed lookup services on peer-to-peer networks. CFS \cite{dabek} and PAST \cite{rowstron01a} implement read-only systems using content-based addressing, similar to Venti \cite{quinlan}, but using an underlying DHT to locate object replicas. Other systems implement mutable file systems over similar substrates, storing data blocks, files, or content-based fragments \cite{rabin81} as immutable objects distributed across the network. Pasta \cite{moreton} stores metadata in special index blocks, each of which is associated with an asymmetric cryptographic key. The key is used to locate the index block (instead of using a hash of the contents as for normal data blocks) and to protect changes to it, allowing the index to change while retaining a unique static ID. Eliot \cite{stein02} stores metadata outside the immutable substrate, creating a separate, writable system that references the read-only data indexed by the DHT. Ivy \cite{muthitacharoen02} stores all user changes to the user's log, which is then made available to others through the DHT, and each user consults as many logs as necessary to construct a coherent view of the file system.

Peer-to-peer file systems all suffer from the transience of users. Volunteered resources can be withdrawn without notice, so high levels of replication are required to ensure accessibility and availability \cite{blake,rabin89}. Latency is also high in the wide area, so further replication and caching is necessary to make performance acceptable. OceanStore \cite{kubiatowicz} was designed as a global network of highly-connected clusters that cooperate closely, with additional clustering at the file level for groups of files that are regularly accessed together. The prototype, called Pond \cite{rhea}, uses Tapestry to organise virtual resources (data blocks and manager nodes), but it also forms localised clusters of participants to implement Byzantine agreement protocols without the high latency typical of DHTs. These systems are tiered to reclaim some of the benefits of locality while still providing a global service.

Envoy has some structural parallels with these systems. While latency between machines in a cluster is much lower, Envoy still caches data on disk and in memory near the client. Locality is pursued at the machine level to aggregate the storage requirements of a set of virtual machines, with data ultimately replicated and spread throughout the cluster as storage objects.

The relationship of the present work to global storage systems is more than just a passing architectural resemblance, however. Envoy and the service clusters that host it are designed for commercial providers that locate computational resources near storage and fast network access. Instead of trying to hide the distance between users and data, service clusters share the goal of XenoServers \cite{reed} to move computation to a resource-rich environment. As a basic platform, service clusters backed by Envoy storage can form the building blocks of global service networks, including storage services.

\subsection{Cluster storage systems}\label{sec:cluster-storage-systems}

The path from local file systems to wide-area file systems is generally progressive, with concerns about cache management, trust, latency, replica placement, and reliability growing at each step. Clusters have networks with low latency and high bandwidth, and large numbers of trusted, centrally-managed hosts. Their mixture of characteristics from the largest and smallest of systems partially explains their popularity as replacements for traditional, monolithic supercomputer architectures.

With a large number of redundant components, clusters have the potential to be highly reliable (in the aggregate) with abundant bandwidth. One of the principal drivers of cluster storage systems is avoiding bottlenecks. While this is important in all systems, the vast aggregate disk and network bandwidth available to a large cluster makes it easy to overwhelm any single component \cite{hospodor}. In addition, the opportunity cost of wasted resources is potentially very high, so cluster systems must make good use of the combined storage capacity and I/O bandwidth available from the array of component machines.

\subsubsection{Storage layers}\label{sec:storage-layer-systems}

A common approach for cluster storage systems is to divide the problem into two distinct layers: one that manages the physical disks, balancing load and capacity and handling the addition and loss of disks, and a second layer that builds a file system above an abstract block- or object-level interface provided by the first.

Zebra \cite{hartman93} stripes data across multiple storage servers in a network version of RAID \cite{patterson}, but uses a single file manager to coordinate metadata. Zebra is based on the Sprite LFS implementation, and clients cache data and metadata the same as they would in Sprite. The file manager assumes the role of a standard file server, but all log operations are striped and clients direct data requests directly to the storage servers. Swarm \cite{hartman99} removes the file manager to present a stand-alone layer that exports striped logs to clients, which can be used to implement local file systems or other high-level services.

Petal \cite{lee95,lee96} pools storage devices to export sparse virtual block devices to clients. Persistent state is distributed across the servers, and global state is updated using a distributed agreement protocol that tolerates node failures. By caching a small amount of metadata, clients can direct most requests directly to the correct storage server, updating metadata lazily when it proves to be out-of-date. Petal maintains a global map of servers for each virtual device, optionally using \emph{chained declustering} for redundancy. Chained declustering \cite{hsiao} allows simple load redistribution when nodes fail and makes catastrophic failures more likely to damage a few virtual disks extensively than cripple many virtual disks with small corruptions.

Frangipani \cite{thekkath} builds a file system over a Petal virtual disk. It takes advantage of the large, sparse address space to simplify data structures, with large regions reserved for inodes, allocation bitmaps, logs, small blocks, and large blocks. As with most layered systems, the physical layout is not determined by the virtual layout, so many of the layout strategies of local file systems are inapplicable \cite{stein05}. Instead, the layout of files is simple, with a map tracking up to sixteen 4~KB blocks for small files, spilling into one of $2^{24}$ large blocks that can store files up to 1~TB. File systems can be shared by multiple clients; a distributed lock manager coordinates caching and prevents corruption from concurrent access.

Object-based storage systems \cite{factor,mesnier} provide an object-level interface to disk space. Instead of exporting virtual disks to clients, they manage storage as collections of objects with attributes, which generally correspond directly to files and directories in complete file systems. Network-attached secure disks (NASD) drop storage servers and embed management functions directly in the storage device to save costs \cite{gibson97,gibson98a}, while metadata is controlled by a separate server. They require a separate metadata server to manage security and coordination for shared storage, but cryptographic techniques make these metadata operations largely asynchronous. Synchronous communications are mainly confined to direct requests from clients to the storage devices. 

FAB \cite{frolund,saito04} refines the ideas found in Petal and introduces the idea of \emph{storage bricks}---dedicated storage modules made from commodity components that can be combined to form a storage pool. FAB randomly assigns each data segment to a set of storage bricks and uses majority voting to manage replicas. This allows it to tolerate failures and add new bricks without pausing and temporarily ignore overloaded servers. It also supports fast snapshot operations without a centralised coordinator \cite{ji}.

The Self-* Storage project aims to extend brick-based storage to automate many aspects of configuration and management of storage in order to reduce administrative hassle and costs \cite{ganger03}. Like the AutoRAID storage device \cite{wilkes95}, the Ursa Minor prototype \cite{abd-el-malek} focuses on optimising data layout and failure tolerance for different workloads and requirements. Using a unified infrastructure it supports multiple storage policies and can convert existing data while still operating normally.

Storage layers are complementary to the goals of Envoy. Indeed, the Envoy prototype's primitive storage layer is a stub intended to be replaced by a system much like those described here. Envoy assumes a reliable and performant storage layer, and builds a complete distributed file system optimised for large numbers of untrusted virtual machines above it.

\subsubsection{File systems}

Built on storage layers that can deliver parallel access to many disks, cluster file systems are left mainly with problem of managing metadata and maintaining cache consistency. Lustre \cite{lustre} follows the model proposed by the NASD group \cite{gibson98a}, with clients contacting storage devices directly for object-level data access. Metadata is controlled by a centralised server with a failover replica.

The Google File System \cite{ghemawat} focuses on a specific workload and drops the POSIX file system interface. It, too, splits metadata management from data storage, but is further optimised for large files with sequential reads and append-only writes. Random reads and writes are supported, but the overall emphasis is on high throughput rather than low latency. In this environment, data caching has little value and metadata is minimised by managing files as sequences of large, 64~MB chunks. Chunks are stored as normal files using a Linux file system on commodity hardware.

Clusters are widely used as replacements for supercomputers, and many cluster file systems are tuned to scientific-computing workloads. Like the Google environment, they often require high sustained throughput from large files. Unlike workstation environments where read-write sharing is rare, scientific computing makes frequent use of parallel processes writing to different parts of the same large file \cite{wang04}. GPFS \cite{schmuck} is optimised for large deployments where any centralised coordination point is unacceptable. Based on a block-level storage system, it supports a high degree of concurrency by replacing the centralised metadata manager with a distributed lock manager with byte-range locking, and using extensible hashing to support large directories that can be queried and updated concurrently. In addition, it allows write sharing for non-conflicting metadata updates and pushes most of the communication burden for token revocation to the client triggering it, further reducing synchronous metadata operations in the lock manager.

Many commercial solutions use dedicated storage area networks (SAN) to connect hosts with storage devices over a high-speed switching fabric. As with most of the systems described here, SANs let clients connect directly to storage devices and rely on a separate layer to mediate sharing and form a file system. Storage Tank \cite{menon} uses manually-configured partitions of the namespace to distribute control between metadata servers and relies on the block-based interface provided by SANs, but the overall structure is similar to systems like GPFS.

Ceph \cite{weil06} is also tuned to scientific cluster workloads, but is based on object storage. It, too, distributes metadata management to avoid bottlenecks, but instead of using distributed locks and having clients directly modify metadata structures, it builds a distributed version of the metadata manager common in other object-based systems. Object replicas are placed according to a special hash function, allowing clients to compute the location of an object instead of having to consult the metadata manager. Ceph handles the problem of heavy write sharing in scientific workloads by augmenting the POSIX interface to allow weakened consistency semantics. This works for scientific computing, where applications are generally custom-written anyway, but is less helpful when clients use commodity tools. Like Envoy, Ceph divides management of the hierarchical namespace according to runtime activity, but it does so purely for load balancing within the distributed metadata service \cite{weil04}; no server benefits more than another from controlling a particular region of the namespace. Envoy, in contrast, delegates metadata management to the client dominating access to that part of the file system, making the placement of metadata management a matter of absolute performance as well as load distribution. It also differs from Ceph's distributed scheme by using a time-based protocol to prioritise changes and promote stability (see \secref{sec:territory-management}).

While Envoy is also built for clusters, it is designed for many independent clients with more traditional workloads, rather than large, scientific systems bringing the power of thousands of machines to bear on a single data set. Envoy creates a single hierarchical namespace, but it is arranged as an administrative tree with discrete, client-level file systems as leaves. Envoy is optimised to manage large numbers of these images, with full administration of a private image being managed by the machine using it, and control of shared images being distributed among the participating clients according to runtime demand. In this way, Envoy---with its model of a cluster being host to more clients than machines---is largely complementary to the systems described here, which attempt to make the cluster act like a single, large machine. Instead of centralising metadata management and then introducing distribution schemes to overcome the limitations of centralisation, Envoy partitions the namespace according to runtime demand and distributes metadata control to the clients, or more precisely to a secure virtual machine hosted on the same node as the client.

\subsection{Virtual machines}

A few storage systems have been designed specifically for virtual machine environments. Operating systems hosted on virtual machines can use conventional file systems implemented on private block devices. These can be physical partitions assigned to individual virtual machines, or virtual block devices accessed through a virtualized driver. In addition, network-facing client protocols such as NFS operate the same on virtual machines as on real machines.

My experience with two systems influenced the design of Envoy. The first was CoWNFS, a stacking file system that uses copy-on-write techniques and user-controlled layers to emulate snapshots and file system forks \cite{kotsovinos04b}. CoWNFS operates as a userspace NFS server running in an administrative virtual machine and exporting customised storage views to clients. A private, writable layer can be stacked over a read-only template to capture changes and isolate them from other users. Sharing is possible between clients through the NFS protocol, but access is limited to files already available in the namespace of the administrative VM.

The second system, Parallax \cite{warfield}, exports private virtual block devices to clients. Like CoWNFS and Envoy, Parallax operates through a server in the administrative VM on each host. Template images with fork and snapshot support assist rapid deployment of clients, using a copy-on-write mechanism as clients diverge from their starting templates. Parallax operates at the block level with no support for sharing between clients, except when clients inherit read-only blocks from the same template. A distributed block-storage layer makes use of cheap commodity disks in the host machines for persistence and location-transparent access.

Ventana \cite{pfaff} is a file system for virtual machine environments with similar goals to Envoy. Like Parallax and CoWNFS, both systems provide a single server for each machine, which clients access through a virtual network or block device. Ventana also supports snapshots and forks of file system images, and tracks versioning at the per-file level. Like Envoy, it uses an object-based storage layer, but objects are immutable and changes are tracked through successive file versions, similar to JetFile \cite{gronvall}. Ventana uses a centralised metadata server to track file versions (unlike JetFile, which announces new versions through IP multicast) and to manage image branches. It offers loose consistency, communicating with clients over the NFSv3 protocol and requiring nodes to check with the metadata server on each access to bound cache divergence. Persistent caching at each node permits disconnected operation for clients, and any resulting conflicts are managed by applying all client changes to the repository after making a snapshot. To lessen the bottleneck of a centralised metadata server, Ventana allows file system images to be marked as private and all metadata to be cached at the node. Envoy targets clusters of virtual machines where centralised servers are impractical. Metadata control is distributed, and manual configuration of private and shared images is rejected in favour of dynamic algorithms that adapt to runtime behaviour.

Virtual machines have also been used to support grid computing \cite{figueiredo03}. GVFS \cite{zhao04} extends NFS with userspace proxies that implement persistent caching, file prefetching, and per-file metadata hints, allowing compressed transfers and other deviations from standard NFS semantics to better support the wide-area grid architecture. Like other virtual machine storage systems, GVFS exploits the redundancy in file system images cloned from a template, using symbolic links to introduce some transparency in the cloning process and facilitate shared caching.

\section{Summary}

Amazon EC2 \cite{amazon}

Palimpsest \cite{roscoe03}

Multi-Service Storage Architecture \cite{bacon}