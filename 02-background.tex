\chapter{Background}

Storage systems have been studied and documented extensively in the systems literature. One could argue that everything worth doing has been done, or alternatively one could assert that the continuing research is evidence that everyone has failed so far, with the truth probably residing somewhere between these two extremes. Storage is an essential component of most systems, though, and the requirements placed on them are almost as various as the solutions that have been proposed.

In this chapter I survey the most important and widely known storage systems, with an emphasis on those that are relevent to service clusters and similar in design to Envoy. I describe some of the major design tradeoffs that emerge and identify the characteristics of service clusters that make existing systems unsuitable or less than ideal for this emerging environment as I justify the need for another storage system.

\section{Commodity computing}

Similar idea as network appliances: \cite{sapuntzakis03}. SoftUDC: software-based data center \cite{kallahalla}. Utilification \cite{wilkes04}.

\subsection{Network of workstations}
Networks of workstations (NOW)\cite{anderson95a}

\subsection{Clusters}

Embarassingly parallel problems

CARD for cluster monitoring\cite{anderson97}

Fast crash recovery verses redundancy\cite{baker94}

Partition cloning \cite{rauch}

TranSend was written for and the Inktomi search engine adapted to a layered cluster platform for scalable, homogeneous network services. Services composed of a few distinct types of actors could be managed by a common platform that provided monitoring and allocation of the component parts over the cluster. This type of platform exemplifies many of the advantages of clusters for network services, including load balancing, incremental scaling, fault tolerance, and high availability, but the implementation was limited to ``embarassingly parallel'' problems with trusted components and a custom state management system backed by a commercial SAN \cite{fox}. The authors were able to demonstrate that scalability was limited more by bandwidth into the cluster than by communication within it or coordination overhead.

\subsection{The GRID}
\cite{zhao}

\subsection{Hosted services}

\section{Virtualization}

\subsection{Virtual machine monitors}
IBM System/370 \cite{gum}
Disco
Denali
Xen \cite{barham}
Live migration \cite{clark} \cite{sapuntzakis02}
Terra \cite{garfinkel}
VMWare
vMatrix
IBM Managed Hosting

\subsection{XenoServers}
\cite{kotsovinos}

Ventana \cite{pfaff}

\section{Storage systems}

Storage is a fundamental component of all general-purpose computer systems. A combination of high storage density, random access, and low cost have made magnetic storage on rotating platters the dominant medium for durable storage. The time to access a random byte of data from a disk is typically measured in milliseconds, while for DRAM this time drops roughly six orders of magnitude to a nanosecond scale, and that gap is continuing to grow.

Because disks are so slow, storage systems are designed around the goal of accessing the disk as little as possible, and favoring sequential access to avoid costly seek delays. Effective use of cache is vital to this goal, and specialized storage systems tuned to specific system architectures and expected workloads are worthwhile because of the potential speed gains over more general systems.

Storage systems have been studied extensively, and one purpose of this section is to survey related work that has influenced this dissertation. A second purpose relates to the thesis of this work, which argues that a new environment justifies a new storage architecture. The works discussed here are presented in the context of the environments that inspired them, in part to establish a pattern of symbiosis between storage architectures and computation environments, a pattern at the heart of the present work.

\subsection{Local file systems}

Disks are mechanical devices and their performance is limited primarily by the need to physically position the head over the correct part of the platter to access data. File systems designed for local disks achieve performance by minimizing the physical movement required. Disk access can be reduced through caching, and latency penalties can be minimized through strategic data placement.

The Berkeley Fast File System (FFS) was the first to optimize data layout for performance by clustering related information. Block metadata is distributed across the disk to be near the file data it describes, file metadata is grouped for files in the same directory, and blocks in the same file are grouped whenever possible \cite{mckusick}. Similar concepts have extended beyond local file systems; the same expectations of correlated access can be exploited to reduce latency when accessing data across the network.

File system tracing reveals how files are accessed in real systems. Most files accessed are small, but size distributions are skewed enough that most data transferred is from large files. Writes are less common than reads, and most files are short-lived. Files access is typically sequential, and most files are read from or written to in their entirety \cite{ousterhout,ruemmler,gibson98b}.

As cache sizes grow, more requests can be satisfied without consulting the disk, and file system designs can assume that commonly-used metadata is resident in memory. At the extreme, most read requests can be satisfied by a large cache, but all writes have to go to disk eventually (with the exception of data that is deleted or overwritten before it is commited to disk). Writes requests are fewer in number, but the mix of operations that penetrates the cache and reaches the disk has a different composition than those initiated by clients. Furthermore, updates often involve metadata changes as well, requiring multiple disk writes even for small updates. a Log-structured file systems (LFS) responded to this observation by making the entire file system an append-only log to optimize write operations \cite{rosenblum}. Seltzer et al.\ \cite{seltzer} compared FFS with LFS and found these two radically different designs had surprisingly comparable performance overall. Each excelled under certain conditions, but both could be characterized by the disk head movements they required to satisfy requests.

As Stein argued \cite{stein05}, the low-level layout issues that drive much of local file system design are obscured in larger systems, particularly those involving multiple disks. Disk reliability depends on redundancy \cite{patterson}, so multiple disks are standard for important data. With distributed file systems that often takes the form of replication across multiple hosts, but even in local systems a file system optimized around the movements of a disk head becomes less predictable when the single-disk model is not adhered to.

% TODO: This is a really weak section

Elephant? \cite{santry}

\subsection{Client-server file systems}

If local file systems can be characterized by how they manage disk head motion, distributed file systems are dominated by concerns of data placement and cache management. For a single host, cache management is mainly concerned with deciding when to commit writes to disk to ensure durability in the face of a system crash, but distributed systems must also consider cache consistency in the face of concurrent access. ``Concurrent'' in this case includes overlapping cache entries as well as in-flight requests that potentially conflict.

The Sun Network File System (NFS) was the first widely-used file system for sharing files across hosts \cite{sandberg}. NFS serves many clients from a single server, which hosts the persistent and canonical version of a file. Cache policy is unspecified, with no explicit support from the server. Clients generally cache files in memory and periodically check with the server to detect updates from other clients that would invalidate the cache entry. Thus an update made by one client is only detected by another when the first has sent the update to the server and the second has checked for an update, prompting clients to send a constant stream of \texttt{stat} requests to check for updates, but leaving them with no way to hasten a delayed update from another client. An update to the protocol helped this somewhat by performing implicit \texttt{stat} requests and including the results with common operations, but the fundamental problem remained \cite{pawlowski,callaghan}.

Other client-server systems addressed the problem in different ways. The Andrew File System (AFS) \cite{satyanarayanan85,howard} and its successor Coda \cite{satyanarayanan90} use the client's disk as a persistent cache with close-to-open semantics. In this system, the cache is validated at file open time, and changes forwarded to the server at file close time. To reduce validity checks, a client could be given a \emph{callback}, meaning that the server would notify it if a change was made to the file by another client. These changes improved scalability by reducing the load on the server, but they still left open the possibility of conflicting updates on multiple clients.

The Sprite team observed that most applications do not explicitly account for the possibility of inconsistency, so they sacrificed some performance for full cache consistency by disabling caching for files under contention \cite{baker91,nelson,welch}. Since concurrent access is relatively rare \cite{kistler}, this did not pose a significant problem for overall performance.

LOCUS \cite{walker}
Cedar \cite{gifford,hagmann}
NFSv4
Alpine \cite{brown}
Spring \cite{khalidi}
Harp \cite{liskov}
Plan 9 \cite{pike90} name spaces \cite{pike92} in Linux with v9fs \cite{hensbergen}
Venti \cite{quinlan}

\subsubsection{Caching}
\cite{dahlin94a}\cite{chaiken}\cite{blaze}\cite{dahlin94b}\cite{keleher}\cite{muthitacharoen}. 1\% of files are used daily, 90\% never used after creation \cite{gibson98b}.

\subsubsection{Disk layout}
Zebra \cite{hartman93} is network RAID \cite{patterson}, and Swarm \cite{hartman99} isn't far off. AutoRAID \cite{wilkes95}. Block-based sharing \cite{mcgregor}

\subsection{Serverless file systems}

While creative caching can alleviate the problem somewhat, all client-server architectures have inherent scaling problems. As a single point of contact for all clients, a server's load grows in proportion to the number of clients, and it also represents a single point of failure. In addition, the duties of a server tend to make it unsuitable for other uses, so such an architecture calls for a dedicated server. As workstations became more powerful, harnessing their excess capacity to cooperate on large problems became attractive \cite{anderson95a}.

In xFS, the traditional roles of a server are split and distributed to the clients to yield a serverless architecture \cite{wang93,anderson95b}. Hosts can act as clients, managers that coordinate data placement, and/or storage servers that provide disk space. Cache coherency is achieved through an explicit consistency protocol where conflicting clients are detected and managed. In addition, the xFS team observed that modern networks make retrieval from a peer's cache faster than from a disk, so sharing and coordinating cache across hosts can yield benefits over discrete local caches. The resulting protocol was so complex that the team had to employ a formal protocol verifier to get a working implementation \cite{wang98}. While a valid way to manage complexity, reducing complexity is preferable, especially in storage systems where correctness is paramount.

Farsite \cite{bolosky,adya}\cite{douceur01}\cite{douceur02}

\subsection{Wide-area file systems}

Prospero \cite{neuman}
Oceanstore and Pond \cite{kubiatowicz,rhea}

The Echo distributed file system \cite{birrell93} was designed to provide a single, global namespace that was accessible anywhere and perfectly coherent. It uses write-behind caching for files and directories with ordering constraints to preserve consistency even in the presence of crashes \cite{mann}, and employs a token-passing scheme to manage cache coherency. The top levels of the namespace are hosted on the DNS, with a link to a specific server. From there, servers host branches of the tree with further links to additional servers. Replication is done at the server level with primary and secondary servers that are synchronized. It died when the OS that hosted it wasn't ported to newer hardware (another reason to use commodity systems).

Ficus \cite{popek}

\subsection{Peer-to-peer file systems}

Pasta \cite{moreton}
Pangaea \cite{saito02}
\cite{stein02}
JetFile \cite{gronvall}

For file distribution, read-only file systems \cite{fu} and bittorrent, etc.

To be reliable you need lots of replicas \cite{rabin}, probably too many \cite{blake}.

%\subsection{Mobile computing support}
\cite{kim}\cite{mummert}\cite{sobti}

\subsection{Clusters}

\cite{amiri}

Layered clustering in Cuckoo allows NFS servers to be combined into a cluster system \cite{klosterman}, spreading load, etc. Mirage uses a similar approach to aggregate multiple NFS servers into a single, virtual server \cite{baker02}.

Petal \cite{lee95,lee96}
Frangipani \cite{thekkath}
Ursa Minor \cite{abd-el-malek}
Google file system \cite{ghemawat}
Lustre \cite{lustre}
Parallax \cite{warfield}
FAB \cite{frolund,saito04,ji}
Self-* Storage \cite{ganger}
Ceph \cite{weil}

\subsection{Storage area networks}
\cite{burns}. Bandwidth can become a problem as scale grows \cite{hospodor}, requiring careful architecture choices.

Network-attached secure disks (NASD) \cite{gibson97,gibson98a}

Network-attached storage (NetAPP) \cite{hitz}

\section{Summary}
