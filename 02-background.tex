\chapter{Background}



\section{Commodity computing}

Similar idea as network appliances: \cite{sapuntzakis03}. SoftUDC: software-based data center \cite{kallahalla}. Utilification \cite{wilkes04}.

\subsection{Network of workstations}
Networks of workstations (NOW)\cite{anderson95a}

\subsection{Clusters}

Embarassingly parallel problems

CARD for cluster monitoring\cite{anderson97}

Fast crash recovery verses redundancy\cite{baker94}

Partition cloning \cite{rauch}

TranSend was written for and the Inktomi search engine adapted to a layered cluster platform for scalable, homogeneous network services. Services composed of a few distinct types of actors could be managed by a common platform that provided monitoring and allocation of the component parts over the cluster. This type of platform exemplifies many of the advantages of clusters for network services, including load balancing, incremental scaling, fault tolerance, and high availability, but the implementation was limited to ``embarassingly parallel'' problems with trusted components and a custom state management system backed by a commercial SAN \cite{fox}. The authors were able to demonstrate that scalability was limited more by bandwidth into the cluster than by communication within it or coordination overhead.

\subsection{The GRID}
\cite{zhao}

\subsection{Hosted services}

\section{Virtualization}

\subsection{Virtual machine monitors}
IBM System/370 \cite{gum}
Disco
Denali
Xen \cite{barham}
Live migration \cite{clark} \cite{sapuntzakis02}
Terra \cite{garfinkel}
VMWare
vMatrix \cite{awadallah}
IBM Managed Hosting

\subsection{XenoServers}
\cite{kotsovinos}

Ventana \cite{pfaff}

\section{Storage systems}

Storage is a fundamental component of all general-purpose computer systems. A combination of high storage density, random access, and low cost have made magnetic storage on rotating platters the dominant medium for durable storage. The time to access a random byte of data from a disk is typically measured in milliseconds, while for DRAM this time drops roughly six orders of magnitude to a nanosecond scale, and that gap is continuing to grow.

Because disks are so slow, storage systems are designed around the goal of accessing the disk as little as possible, and favoring sequential access to avoid costly seek delays. Effective use of cache is vital to this goal, and specialized storage systems tuned to specific system architectures and expected workloads are worthwhile because of the potential speed gains over more general systems.

Storage systems have been studied extensively, and one purpose of this section is to survey related work that has influenced this dissertation. A second purpose relates to the thesis of this work, which argues that a new environment justifies a new storage architecture. The works discussed here are presented in the context of the environments that inspired them, in part to establish a pattern of symbiosis between storage architectures and computation environments, a pattern at the heart of the present work. Envoy, the file system introduced in this dissertation, is discussed briefly in this section to relate it to prior work, but details of its design are reserved for later chapters.

\subsection{Local file systems}

Disks are mechanical devices and their performance is limited primarily by the need to physically position the disk head over the correct part of the platter to access data. File systems designed for local disks achieve performance by minimizing the physical movement required. Disk access can be reduced through caching, and hardware latency can be minimized by arranging data on the disk to minimize movement. Correctness and reliability tend to trump performance as design considerations, however, and the wide variety of possible workloads makes it difficult to pick clear winners from competing designs.

The Berkeley Fast File System (FFS) was the first to optimize data layout for performance by clustering related information. Block metadata is distributed across the disk to be near the file data it describes, file metadata is grouped for files in the same directory, and blocks in the same file are grouped whenever possible \cite{mckusick}. \emph{Clustering}, as this design is called, exploits concepts that extend beyond local file systems: the same expectations of correlated access can be exploited to reduce latency when accessing data across the network.

File system tracing reveals how files are accessed in real systems \cite{ousterhout}. Most files accessed are small, but size distributions are skewed enough that most data transferred is from large files. Writes are less common than reads, and most files are short-lived. Files access is typically sequential, and most files are read from or written to in their entirety. These trends have proved resiliant over time, though the scale has increased and largest files in typical systems have grown much larger \cite{ruemmler,gibson98b}.

As cache sizes grow, more requests can be satisfied without consulting the disk, and designs can assume that many read requests can be served from memory. All writes whose effects are not quickly undone have to go to disk eventually, so despite being fewer overall, writes requests can dominate the mix of operations that penetrates the cache and reaches the disk. Furthermore, updates often involve metadata changes as well, potentially requiring multiple costly disk seeks even for small updates.

The log-structured file system (LFS) addressed this problem by borrowing from database design and making the entire file system an append-only log. Writes are gathered and written as sequential chunks on disk, with relevant metadata rewritten instead of updating existing structures directly \cite{rosenblum}. \emph{Journaling} file systems apply the same idea to other file system types, logging only the intent to update metadata. Once the log is committed, the conventional structures can be updated asynchronously while still guaranteeing durability in the face of a system crash \cite{hagmann,sweeney,tweedie}. Breaking the chain of required metadata commits is also the objective of \emph{soft updates}, a technique that involves careful rearrangement of data in the buffer cache to permit delays and reordered writes \cite{ganger94}.

Studies comparing FFS with LFS \cite{seltzer95} and journaling with soft updates \cite{seltzer00} reveal a complicated picture. Each excels under certain workloads, but no clear winner emerges for all or even most scenarios. With appropriate techniques, good cache management can mask latency even for writes.


% TODO: This is a really weak section
What's the point?  How does this apply to Envoy?

In distributed systems or even local RAID arrays \cite{patterson}, higher-order effects can further distort the picture \cite{stein05}.

Elephant? \cite{santry}

\subsection{Client-server file systems}

If local file systems can be characterized by how they manage disk head motion, distributed file systems are dominated by concerns of data placement and cache management. For a single host, cache management is mainly concerned with deciding when to commit writes to disk to ensure durability in the face of a system crash, but distributed systems must also consider cache consistency in the presence of concurrent access. If a cache delivers an out-of-date version of a file, the application may be led to produce incorrect results.

The Sun Network File System (NFS) was the first widely-used file system for sharing files across hosts \cite{sandberg}. NFS serves many clients from a single server, which hosts the persistent and canonical version of a file. Cache policy is unspecified, with no explicit support from the server. Clients generally cache files in memory and periodically check with the server to detect updates from other clients that would invalidate the cache entry. Thus an update made by one client is only detected by another when the first has sent the update to the server and the second has checked for an update, prompting clients to send a constant stream of \texttt{stat} requests to check for updates, but leaving them with no way to hasten a delayed update from another client. An update to the protocol helped this somewhat by performing implicit \texttt{stat} requests and including the results with common operations \cite{pawlowski,callaghan}, but the fundamental problem remained as most clients still delay writes to the server.

Other client-server systems addressed the problem in different ways. The Andrew File System (AFS) \cite{satyanarayanan85,howard} and its successor Coda \cite{satyanarayanan90} use the client's disk as a persistent cache with close-to-open semantics. In this scheme, the cache is validated at file open time, and changes forwarded to the server at file close time. To reduce validity checks, a client can be given a \emph{callback}, meaning that it can assume the file is current unless explicitly notified by the server. These changes improved scalability by reducing the load on the server, but they still left open the possibility of conflicting updates on multiple clients.

The Sprite team observed that---despite the popularity of such systems---most applications do not explicitly account for inconsistencies introduced by the file system, so loosening consistency guarantees in favor of performance gains is a dangerous tradeoff. They sacrificed some performance for full cache consistency by disabling caching for files under contention \cite{baker91,nelson,welch}. Since concurrent access is relatively rare \cite{kistler}, this did not pose a significant problem for overall performance.

In all distributed file systems that permit sharing, there must either be a canonical version of the file (or block \cite{mcgregor}) or some way to reconcile conflicting updates \cite{kistler}. In the former case, some participant is usually nominated as the owner of a particular file through a lease \cite{gray89}, token passing \cite{burrows,mann}, or some other scheme. Any other host wanting access to the latest version must coordinate through the owner. Ownership may go to the host that provides storage, the one actively using the file, or a third manager host that connects the two \cite{blaze,keleher}. In Envoy, ownership goes to an active user, which then acts as a synchronous server to other users. Unlike Sprite, the principle user can continue to cache the file locally and share its cache with other concurrent users.

Another possibility is to disallow write sharing. The Cedar file system \cite{schroeder} makes all shared files immutable, and turns the problem into one of versioning \cite{gifford}. Venti takes this a step further by storing all file versions permanently and addressing them by a hash of their contents \cite{quinlan}. In workstation environments, it is possible for storage capacity to grow faster than storage is consumed, making this a feasible system. A less drastic approach is to permit snapshots \cite{hitz}, then recognize that historic versions are read-only and do not need cache coordination \cite{warfield}. Envoy employs this dichotomy between active and read-only file versions, implementing cache coordination only for mutable objects. It does not coalesce identical read-only objects like Venti, though it could be extended to do so using a lazy, asynchronous process.

Where snapshot systems typically use copy-on-write to transparently combine old data with new, some systems allow explicit stacking of file systems. Spring \cite{khalidi} allows file systems to be layered with optionally synchronized updates as a mechanism for extending functionality by layering in new features. Plan~9 \cite{pike90} allows any file system mount to be layered over another and their contents combined to give each user a custom view of local and remote file systems \cite{pike92}. The copy-on-write NFS server I developed for the XenoServers project \cite{kotsovinos} allowed layering instructions to be put in files, where the server would immediately notice them and reconfigure the user's view of the file system. These systems can be used as a way to fork a file system, by sharing the common base image and capturing changes in a private layer. Over time this can lead to complex hierarchies of layers, and such systems rely on the semantics of their backing file systems. Envoy can be used in conjunction with a stacking layer, but it provides explicit support for snapshots and file system forks.

DFS \cite{kazar}
NFSv4
%Alpine \cite{brown}
Harp \cite{liskov}

\subsection{Serverless file systems}

While creative caching can alleviate the problem somewhat, all client-server architectures have inherent scaling problems. As a single point of contact for all clients, a server's load grows in proportion to the number of clients, and it also represents a single point of failure. In addition, the duties of a server tend to make it unsuitable for other uses, so such an architecture calls for a dedicated server. As workstations became more powerful, harnessing their excess capacity to cooperate on large problems became attractive \cite{anderson95a}.

In xFS, the traditional roles of a server are split and distributed to the clients to yield a serverless architecture \cite{wang93,anderson95b}. Hosts can act as clients, managers that coordinate data placement, and/or storage servers that provide disk space, similar to the file system of the earlier LOCUS distributed operating system \cite{walker}. Cache coherency is achieved through an explicit consistency protocol where conflicting clients are detected and managed.

In addition, the xFS team observed that modern networks make retrieval from a peer's cache faster than from a disk \cite{dahlin94b}, so sharing and coordinating cache across hosts can yield benefits over discrete local caches. The resulting protocol was so complex that the team had to employ a formal protocol verifier to get a working implementation \cite{wang98}. While a valid way to manage complexity, reducing complexity through design is generally preferable, especially in storage systems where correctness is paramount.

Farsite \cite{bolosky,adya}\cite{douceur01}\cite{douceur02}

The downside of having workstations double as servers is that the server function is not completely isolated from the other activities of the workstation. Runtime activity may be less predictable than on a dedicated server because server load is normally determined by the aggregate activity of many clients instead of that of a single user, and a user may also switch a workstation off without notice. The same trends that lead to excess capacity in workstations make dedicated servers cheap and powerful, without the additional complexity of a heterogeneous management environment. Despite numerous studies \cite{bolosky,douceur99,douceur01} showing feasibility and systems developed and tested \cite{adya,walker}, no serverless file system has seen widespread use for general-purpose computing.

Caching:
\cite{dahlin94a}
Low bandwidth \cite{muthitacharoen}.

Disk layout:
Zebra \cite{hartman93} is network RAID \cite{patterson}, and Swarm \cite{hartman99} isn't far off. AutoRAID \cite{wilkes95}.

\subsection{Wide-area file systems}

Prospero \cite{neuman}
Oceanstore and Pond \cite{kubiatowicz,rhea}

The Echo distributed file system \cite{birrell93} was designed to provide a single, global namespace that was accessible anywhere and perfectly coherent. It uses write-behind caching for files and directories with ordering constraints to preserve consistency even in the presence of crashes \cite{mann}, and employs a token-passing scheme to manage cache coherency. The top levels of the namespace are hosted on the DNS, with a link to a specific server. From there, servers host branches of the tree with further links to additional servers. Replication is done at the server level with primary and secondary servers that are synchronized. It died when the OS that hosted it wasn't ported to newer hardware (another reason to use commodity systems).

Ficus \cite{popek}

\subsection{Peer-to-peer file systems}

Pasta \cite{moreton}
Pangaea \cite{saito02}
\cite{stein02}
JetFile \cite{gronvall}

For file distribution, read-only file systems \cite{fu} and bittorrent, etc.

To be reliable you need lots of replicas \cite{rabin}, probably too many \cite{blake}.

%\subsection{Mobile computing support}
\cite{kim}\cite{mummert}\cite{sobti}

\subsection{Clusters}

\cite{amiri}

Layered clustering in Cuckoo allows NFS servers to be combined into a cluster system \cite{klosterman}, spreading load, etc. Mirage uses a similar approach to aggregate multiple NFS servers into a single, virtual server \cite{baker02}.

Petal \cite{lee95,lee96}
Frangipani \cite{thekkath}
Ursa Minor \cite{abd-el-malek}
Google file system \cite{ghemawat}
Lustre \cite{lustre}
Parallax \cite{warfield}
FAB \cite{frolund,saito04,ji}
Self-* Storage \cite{ganger03}
Ceph \cite{weil}

\subsection{Storage area networks}
\cite{burns}. Bandwidth can become a problem as scale grows \cite{hospodor}, requiring careful architecture choices.

Network-attached secure disks (NASD) \cite{gibson97,gibson98a}

Network-attached storage (NetAPP) \cite{hitz}

\section{Summary}
