\chapter{Service Clusters}

At 9:30~a.m.\ the New York Stock Exchange opens for trading and the computing demands of many financial services companies are focused on the fluctuations of the markets until the closing bell rings at 4:00~p.m. In the hours until the markets open again, the resources of these companies may turn to intensive jobs---such as simulations---for the company itself, or to handling the processing needs of clients. The applications and data sets involved may be completely unrelated to those used during the daytime hours.

Standard business hours drive many other computing patterns as well. A web site will have the highest traffic load when its target audience is awake and in front of a computer. For workers using personal time, this may mean a traffic surge in the morning, another around lunchtime, and a third during the early evening. For email and other services used by businesses, the traffic curve follows business hours, possibly in many time zones. In contrast, game hosts are busier during leisure hours and see many clients otherwise engaged when corporate offices open their doors.

As internet services become more interactive, human clients face competition from mobile agents and other automated services. Amazon recently unveiled a service to open their storage platform directly for use by others for web services. Major internet services like eBay and Google offer APIs for third parties to interact with their services using custom software, rather than just their usual web interfaces for human clients.

All of these examples benefit from the simple and rapid deployment and management of software services in a range of environments. Recent years have seen a renaissance of machine virtualization technology, this time in cheap, commodity hardware.

In this chapter I argue that the combination of virtual machine managers and clusters of commodity hardware offers a potent platform for service deployment, which I call \textit{service clusters}. I further argue that existing storage systems are not well suited for this environment and how it is used, so a new set of requirements and a storage system to match them are called for.

\section{Commodity computing}
\subsection{The flexible commodity computation problem}

\subsection{The GRID}
\subsubsection{Volunteer projects}
\subsubsection{Commercial and research projects}
\subsubsection{Planetlab}

\subsection{Commodity clusters}
\subsubsection{Google}
\subsubsection{Beowolf clusters}
\subsubsection{Supercomputer 500 list}

\subsection{Web services}
\subsubsection{Thin clients}
\subsubsection{Mobile clients}
\subsubsection{Hosted services}
email, payroll, tax prep

\subsection{Flexible commodity computing}
\subsubsection{Morgan Stanley}
\subsubsection{XenoServers}
\subsubsection{Standard platforms}

\section{Clusters as a commodity computing platform}

Clusters of commodity hardware offer several benefits over highly provisioned servers or cooperative but dispersed workstations. They offer a degree of centralization that retains many of the benefits of single servers while adding the robustness and scalability of distributed groups of machines. The onus for maintaining a coherent platform is transfered from the hardware vendor in the case of large servers to the software, but in flexibility achieved in return makes the tradeoff profitible.

\subsection{When one is better than many}

Centralized administration and high-speed communication (via shared memory and IPC) are two advantages of traditional servers. Clusters typically put components in a single location with high-speed local networking and a secure physical location. The machines are all owned and administered by a single organization and can have whatever cooling systems and redundant power supplies are appropriate for the application. Systems designed to prevent or avoid any centralized control usually do so for privacy or legal reasons, neither of which is compelling in the service cluster environment. Indeed, some degree of central administration is an essential feature of service clusters. The owner of the cluster needs to control access and admission to the service pool as well as monitor services that are running to ensure that they do not exceed their alloted resources. Given physical proximity and central control, central administration adds convenience without introducing unnecessary penalties in flexibility.

To further emulate the desirable characteristics of centralized servers, clusters must ensure global access to data in the storage system from any constituent node. This is best achieved through a single, global name space with a completely coherent view of all files in the cluster. Trading coherence for performance represents a failure of global access, as concurrent services effectively create private views of the data, requiring a seperate mechanism for restoring the consistency that the file system violated \cite{birrell93}.

Services running on a single server all fail together if the server fails. Correlated failure is generally considered a problem in distributed systems, but it can also simplify the requirements of failure recovery at least in the common case. When a single server fails, the file system is generally restored to a consistent state before its services are restarted and allowed to recover. If the file system fails while the service continues, this is often considered a similarly catastrophic failure of the system, no better than the two parts failing together. In a virtual machine environmenet, the distributed file system manager can be located on the same physical machine as the service that depends on it, restoring this hardware correlation between the two components. Note that the virtual machine monitor already represents a single point of failure for an entire physical machine; adding the file system manager as a second critical software component that cannot be recovered without a restart of the node only enlarges the set of critical tools, it does not create it. The failure of a client service does not enjoy the same priviledged status.

\subsection{When many is better than one}

Scalability is a significant benefit of clusters over centralized servers. Hardware can be added incrementally according to need, which eliminates the need for an accurate forecast of the lifetime demands of the system. In addition, clusters of machines can achieve much greater overall scale than even the largest single machines. Scalability over time and absolute scalability are also features of non-clustered distributed systems, but those lack the high-speed interconnects possible in managed environments (note that I consider a cluster to be defined more by the coherent administrative environment and high-speed networking than by physical proximity; machines in a single cluster could be located in different buildings or at different sites if the other conditions are met).

The use of commodity hardware also allows rapid machine acquisition and cheap prices. Incremental scaling means that newly added hardware can always take advantage of the best price/performance ratios and benefit from the constant downward pressure on prices in a competitive market \cite{fox}. Commodity disks are relatively unreliable, but they are also large and cheap and offer a good source for storage capacity \cite{patterson,warfield} that comes standard with most machines.

The independence of nodes in a cluster offers redundancy that can be exploited for both reliability and availability. This is necessary not only to exceed the expectations of a single system, but to match it as well. Having many parts that can fail independently offers a much higher probability that at least one will fail than that any single component will fail, and a cluster that does not tolerate some component failures will quickly become unusable \cite{birrell93}.

Clusters have mainly been confined to solving ``embarrassingly parallel''\footnote{See \url{http://en.wikipedia.org/wiki/Embarrassingly_parallel}} problems such as search engines, scientific computing, data mining, and other parallel computing.

\section{Services}

One of the enduring goals of systems research is to provide a good platform for running applications. Even early systems such as Multics were explicitly intended as infrastructure for higher-level computing services, seeking ``continuous operation in a utility-like manner, with flexible remote access,'' with requirements such as ``convenience of manipulating data and program files, ease of controlling processes during execution and above all, protection of private files and isolation of independent processes'' \cite{corbato}. Forty years later, we have made progress but still recognize similar goals.

\subsection{Services as a management unit}

\subsection{Hosting services}

\subsection{Virtual machines}

Hardware virtualization is one of the more potent tools available for achieving these ends. Virtual memory systems are a standard tool for isolating processes managed by operating systems, and virtualizing entire hardware platforms extends the ability to partition resources to the operating system level, which is a more flexible unit of management \cite{hand}. Once confined mainly to specialized server hardware, machine virtualization is emerging as an important general purpose management tool. The Xen hypervisor \cite{barham} was the first to make virtualization efficient for commodity hardware, especially for I/O intensive applications.

\section{Service clusters}

Combining groups of services into clusters yields numerous benefits over deployment in isolation. Load balancing can extend not only to multiple participants in a single logical service, but across dispirate services hosted in the same cluster.

\subsection{Commodity everything}

\subsection{Heterogeneous workloads}

Many web hosting services routinely oversubscribe their hardware capacity, a practice that allows them to provision based on expected average use rather than maximum potential use. Since clusters can be expanded incrementally, observed average use can become the metric for established providers, especially at large scale. It is prudent to hold some reserve capacity to handle bursts in usage, but even burstiness becomes more predictible at large scales. The more diverse the services using a cluster, the less likely an external event will trigger a burst in usage that overwhelms the capacity of the entire cluster. Designing for overflow capability using machines outside the cluster provides insurance against unexpected traffic \cite{fox}, but using a general-purpose service platform with a heterogeneous load allows averaging to reduce the probability of needing resources beyond those in normal use.

Besides accounting for random bursts of traffic, periodic fluctuactions in demand can be exploited to make better use of resources. Predictable events like business hours, holidays, academic calendars, and stable cycles can significantly influence some service workloads. Planning for these by pairing complementary services in a dynamically-configured cluster can help avoid idle resources and reduce expenses. Systems used heavily during business hours could share with systems used by game servers, assuming that the latter are used more during leisure hours than on company time.