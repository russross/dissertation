\chapter{Service Clusters}

The standardization of the shipping container revolutionized the logistics industry, which in turn has had a significant impact on global commerce over the past 50 years. Building a box hardly seems like the stuff of revolutions, but the cheap availability of foreign goods and easy access to global markets that characterizes modern economies owns much to exactly that. The significance of the container is illustrated partly by what it offers, namely, flexibility and efficiency, but equally significant is what it does not offer. It is not an engine, a vehicle, a route, a company, a service, or any of the things necessary to transport goods from one place to another. Instead, it is a neutral, common ground. Those wishing to ship something can pack according to well-known dimensions using widely-available tools, and those offering transport services can use trains, ships, trucks, or whatever method of transport and whatever routing system allows them to offer competitive service while making a profit.

The computer industry is in need of a shipping container. While the computer hardware industry is increasingly viewed as a commodity business, computation as a commodity service is still in its infancy. The components are all there: PCs are powerful, networks are fast, disks are big, operating systems are flexible and efficient, and everything is cheap. The Top~500 supercomputer list is dominated by clusters make from commodity hardware\footnote{See \url{http://www.top500.org/}}, and companies such as Google have used commodity hardware to solve large commercial problems instead of relying on powerful, special-purpose hardware that is more powerful and more reliable, but also much more expensive. These efforts have been highly successful, but they still revolve around using commodity hardware to build platforms that are customized for a particular task or class of tasks. Like oil tankers or passenger trains, they are efficient and well-suited to their intended markets, but not easily adapted to other kinds of clients.

Containers have succeeded for several reasons. They are generic enough to support an enormous range of cargo, but rigid enough to pack tightly together and stack neatly. They can be pulled individually behind trucks, strung together on trains, or packed onto huge cargo ships for ocean transport. They can be moved and loaded easily and quickly using crains, and they can move from one ship to the next without any changes. Clients can fill as many containers as they need and only pay according to what they use.

Many of these same characteristics would help to make commodity computation a reality. Clients should be able to deploy a wide range of computation and communication services, but vendors should also be able to manage them in a generic way. Clients should be able to use their own commodity machines and software to implement services, but have them run equally well in a large, commercial setting. Likewise, deployment costs should be low and procedures simple, and redeployment costs should not create onerous barriers to changing vendors. Finally, small services using very few resources and those spanning many dedicated machines should be able to coexist without interfering with each other and with clients paying according to what they use.

In this chapter I argue that a platform of \emph{service clusters} approximates this ideal, using commodity hardware and commodity operating systems and tools, isolated and managed using virtual machine monitors. The \emph{service} is the unit of management, allowing arbitrary tasks to run, each isolated in a virtual machine. While I do not prescribe a particular operating system or set of tools, I do suggest that a small set of common, commodity platforms can be agreed on, and that clients and vendors can both benefit from adhering to them. Clients can then package their custom services as everything that differs from a standard platform and send it to the vendor that can host it according to their needs.

Not every task is best served by a common platform---oil tankers will continue to excel at transporting oil---but many jobs that today run on custom installations could be better implemented as commodity computation tasks that are cheaper for clients to run and profitable for vendors to host. I discuss other efforts toward commodity computation and how my proposal overlaps or relates to them.

Finally, I do not propose a complete solution. Instead, I argue for the essential characterstics that distinguish service clusters from other large-scale uses of commodity computers, while leaving much of the general infrastructure to others. I focus on the storage needs of the platform and identify how a suitably designed file system can use the commodity hardware in a cluster to cheaply and easily support deployment and management of services built on commodity tools.

\section{The need for a commodity computing platform}

Services are harder to package as commodities than goods. The quality of oil, the purity of precious metals, the strength of steel, and the composition of building materials can all be objectively measured. Compatibility with standards, quality of workmanship, energy consumption, and feature sets make consumer electronic devices comparable, and even food can be graded and compared, at least at the level of basic ingredients. Services can also be commodities, but only when competing offerings can be compared on price and quality, and when customers can move freely between providers without prohibitive lock-in effects.

This section starts by defining computation as a service amenable to commoditization, then explores how this proposal relates to existing platforms and projects with similar goals.

\subsection{The flexible commodity computation problem}

Commodity computing describes a range of systems for exploiting the cheap and plentiful computing power available in commodity PCs. Instead of building expensive, specialty servers to tackle complex problems, commercial users and researchers are increasingly harnessing the power of many smaller, cheaper machines to achieve the same end. Because of the massive economies of scale in the PC market, the aggregate power that can be had from a group of cheap PCs is much greater than what the equivalent funds could purchase in more powerful, specialized server hardware.

Efforts to use commodity hardware for large computation tasks are orthogonal to the goal of treating computation power itself as a commodity. While clusters of commodity machines may be part of the underlying implementation of a commodity computation service, using specialized server hardware is also a viable option. When considering a service as a commodity, the methods used to offer the service are left to the provider, and innovation through proprietary techniques may prove a competitive advantage. From the customer's perspective, it is only the quality of the service rendered and the cost that matter.

\emph{Utility computing}, also called \emph{on-demand computing}, describes the business model of providing computing services and charging based on use of resources. This may apply to specialized services such as databases or web hosting platforms, or to any service where usage is metered and clients pay based on what they use rather than the capabilities that are available. By itself, utility computing answers only part of the problem. Customers are isolated from the fixed costs, risk of component failure, and administrative expenses associated with hardware ownership, but they may be restricted in the range of services offered or applications accomodated. Just as early mainframes required customized software development and incurred porting costs with each new iteration or change of vendor, utility computing systems subject clients to the tools and infrastructure requirements of their hosts.

\emph{Flexible commodity computation} is to a form of utility computing that allows standard commodity operating systems and tools as well as customized services to be deployed quickly and cheaply, where the basic environment provides commodity operating systems and tools rather than a specialized platform. Deployment costs are proportional to how far a client deviates from a standard, commodity environment, e.g., a standard Linux distribution, rather than the total amount of software their application requires to operate. By providing standard tools and standard environments, one flexible commodity computation service can be swapped for another without significant barriers such as porting costs and deployment costs, making computation itself a fungible commodity that can be used for a wide range of tasks, large and small.

\subsection{Commodity clusters}

\subsubsection{Google}
\subsubsection{Beowulf clusters}
\subsubsection{Supercomputer 500 list}

\subsection{The GRID}
\subsubsection{Volunteer projects}
\subsubsection{Commercial and research projects}
\subsubsection{Planetlab}

\subsection{Web services}
\subsubsection{Thin clients}
\subsubsection{Mobile clients}
\subsubsection{Hosted services}
email, payroll, tax prep

\subsection{XenoServers}

Mobile users \cite{demers}

\section{The service as a unit of management}

Services work as commodities when the providers of a specific are interchangible, which requires agreement about not only what is to be done, but what is being acted upon. Shipping firms can offer to transport a container of a specific size and weight between two points for a specific cost. Before the container was standardized, loading and unloading procedures would vary based on what was being shipped, which would affect the cost structure and combine two services that are more efficient when seperated and optimized individually: loading and shipping.

Likewise, a web services platform may offer compelling services for its specific domains, allowing clients to host their web applications easier and cheaper than they could with their own hardware and software stack, but doing so would conflate two issues that could be better optimized individually: providing and managing the hardware resources, and managing the infrastructure for web applications. The expertise required for these two parts of the problem may be quite different, and combining them forces clients to choose a package deal when they may be better served by independent choices. The skills of hardware managers may also be put to better use serving the needs of multiple software platforms, not just web services.

Any domain-specific middleware will necessarily be limited, and coupling the efficiency of a shared hardware infrastructure to a specific application domain will limit the economies of scale that could otherwise be achieved by more specialized providers. True commodity computation will divorce the application domain from the provision of a hosting platform, allowing specialists to excel in serving their respective functions. A platform that supports only a restricted domain of applications is not offering \emph{computation} as its product in the most general sense. Attempting to port service from a client's own machines to a hosted service provider to a competitor's platform will reveal how far each provider is from offering generic computation as a product.

The most flexible host available to clients is wholly-owned and managed hardware. The PC has proved to be extremely adaptable and capable of hosting an enourmous range of applications. Approximating the flexibility of a complete machine gives clients maximum latitude in using a commodity platform, without requiring them to conform to a specific application structure or use specific tools. It also protects the client from being locked-in to a single provider through dependence on proprietary software.


* need the most flexible platform that still allows management, isolation. efficient virtualization of entire machines only recently available on commodity machines, it's the way to go

* VMs let users build and test and even deploy on their own machines, using own tools. using standard OS and toolchain gives same advantages as commodity hardware: cheap, powerful, gets better all the time.

* Structure by services, not by machines. deployment no longer about maximizing use of hardware capabilities---that's a seperate function. instead, run minimum unit (service) in a stripped container (VM on commodity OS + needed tools). mix & match

* package for deployment as a diff from commodity os. just give access to a fresh image for installation, or build infrastructure on top of it to actually package things, let clients start from exact same base image as cluster has

* higher-level services can build on it, differentiate providers, middle-man as a new business, etc. hardware people are good at hardware, middleware people are good at middleware, applications good at applications. what commodities are all about.

One of the enduring goals of systems research is to provide a good platform for running applications. Even early systems such as Multics were explicitly intended as infrastructure for higher-level computing services, seeking ``continuous operation in a utility-like manner, with flexible remote access,'' with requirements such as ``convenience of manipulating data and program files, ease of controlling processes during execution and above all, protection of private files and isolation of independent processes'' \cite{corbato}. Forty years later, we have made progress but still recognize similar goals.

Hardware virtualization is one of the more potent tools available for achieving these ends. Virtual memory systems are a standard tool for isolating processes managed by operating systems, and virtualizing entire hardware platforms extends the ability to partition resources to the operating system level, which is a more flexible unit of management \cite{hand}. Once confined mainly to specialized server hardware, machine virtualization is emerging as an important general purpose management tool. The Xen hypervisor \cite{barham} was the first to make virtualization efficient for commodity hardware, especially for I/O intensive applications.

Using virtual machines makes it easier to manage OSes \cite{chen}. Gartner thinks virtualization is the way to go \cite{bittman}.

Commodity everything, including something as intangible as computation. Storage outsourcing makes sense \cite{ng}.

\section{Service clusters as a commodity computing platform}

Similar idea as network appliances: \cite{sapuntzakis03}. SoftUDC: software-based data center \cite{kallahalla}. Utilification \cite{wilkes04}.

Clusters of commodity hardware offer several benefits over highly provisioned servers or cooperative but dispersed workstations. They offer a degree of centralization that retains many of the benefits of single servers while adding the robustness and scalability of distributed groups of machines. The onus for maintaining a coherent platform is transfered from the hardware vendor in the case of large servers to the software, but the flexibility achieved in return makes the tradeoff profitible.

\subsubsection{When one is better than many}

Centralized administration and high-speed communication (via shared memory and IPC) are two advantages of traditional servers. Clusters typically put components in a single location with high-speed local networking and a secure physical location. The machines are all owned and administered by a single organization and can have whatever cooling systems and redundant power supplies are appropriate for the application. Systems designed to prevent or avoid any centralized control usually do so for privacy or legal reasons, neither of which is compelling in the service cluster environment. Indeed, some degree of central administration is an essential feature of service clusters. The owner of the cluster needs to control access and admission to the service pool as well as monitor services that are running to ensure that they do not exceed their alloted resources. Given physical proximity and central control, central administration adds convenience without introducing unnecessary penalties in flexibility.

To further emulate the desirable characteristics of centralized servers, clusters must ensure global access to data in the storage system from any constituent node. This is best achieved through a single, global name space with a completely coherent view of all files in the cluster. Trading coherence for performance represents a failure of global access, as concurrent services effectively create private views of the data, requiring a seperate mechanism for restoring the consistency that the file system violated \cite{birrell93}.

Services running on a single server all fail together if the server fails. Correlated failure is generally considered a problem in distributed systems, but it can also simplify the requirements of failure recovery at least in the common case. When a single server fails, the file system is generally restored to a consistent state before its services are restarted and allowed to recover. If the file system fails while the service continues, this is often considered a similarly catastrophic failure of the system, no better than the two parts failing together. In a virtual machine environmenet, the distributed file system manager can be located on the same physical machine as the service that depends on it, restoring this hardware correlation between the two components. Note that the virtual machine monitor already represents a single point of failure for an entire physical machine; adding the file system manager as a second critical software component that cannot be recovered without a restart of the node only enlarges the set of critical tools, it does not create it. The failure of a client service does not enjoy the same priviledged status.

\subsubsection{When many is better than one}

Scalability is a significant benefit of clusters over centralized servers. Hardware can be added incrementally according to need, which eliminates the need for an accurate forecast of the lifetime demands of the system. In addition, clusters of machines can achieve much greater overall scale than even the largest single machines. Scalability over time and absolute scalability are also features of non-clustered distributed systems, but those lack the high-speed interconnects possible in managed environments (note that I consider a cluster to be defined more by the coherent administrative environment and high-speed networking than by physical proximity; machines in a single cluster could be located in different buildings or at different sites if the other conditions are met).

The use of commodity hardware also allows rapid machine acquisition and cheap prices. Incremental scaling means that newly added hardware can always take advantage of the best price/performance ratios and benefit from the constant downward pressure on prices in a competitive market \cite{fox}. Commodity disks are relatively unreliable, but they are also large and cheap and offer a good source for storage capacity \cite{patterson,warfield} that comes standard with most machines.

The independence of nodes in a cluster offers redundancy that can be exploited for both reliability and availability. This is necessary not only to exceed the expectations of a single system, but to match it as well. Having many parts that can fail independently offers a much higher probability that at least one will fail than that any single component will fail, and a cluster that does not tolerate some component failures will quickly become unusable \cite{birrell93}.

Clusters have mainly been confined to solving ``embarrassingly parallel''\footnote{See \url{http://en.wikipedia.org/wiki/Embarrassingly_parallel}} problems such as search engines, scientific computing, data mining, and other parallel computing.


Combining groups of services into clusters yields numerous benefits over deployment in isolation. Load balancing can extend not only to multiple participants in a single logical service, but across dispirate services hosted in the same cluster.

\subsection{Heterogeneous workloads}

This is an \emph{advantage}---avoids community effects (stock market, business hours, etc.).

Many web hosting services routinely oversubscribe their hardware capacity, a practice that allows them to provision based on expected average use rather than maximum potential use. Since clusters can be expanded incrementally, observed average use can become the metric for established providers, especially at large scale. It is prudent to hold some reserve capacity to handle bursts in usage, but even burstiness becomes more predictible at large scales. The more diverse the services using a cluster, the less likely an external event will trigger a burst in usage that overwhelms the capacity of the entire cluster. Designing for overflow capability using machines outside the cluster provides insurance against unexpected traffic \cite{fox}, but using a general-purpose service platform with a heterogeneous load allows averaging to reduce the probability of needing resources beyond those in normal use.

Besides accounting for random bursts of traffic, periodic fluctuactions in demand can be exploited to make better use of resources. Predictable events like business hours, holidays, academic calendars, and stable cycles can significantly influence some service workloads. Planning for these by pairing complementary services in a dynamically-configured cluster can help avoid idle resources and reduce expenses. Systems used heavily during business hours could share with systems used by game servers, assuming that the latter are used more during leisure hours than on company time.

Encouraging everyone to use commodity tools and operating systems also increases the capacity of a cluster. The potential savings from shared cache isn't that great amoung unrelated systems \cite{muntz}, but getting everyone to start from a common base image (OS and applications) could improve this.


\section{The storage needs of service clusters}

aka ``Storage requirements''

\subsection{Distribution}

Three basic architectures are available. A centralized server model is the simplest and satisfies the mobility requirement, but it doesn't scale well and suffers from a single point of failure. This model has been extended quite successfully using RAIDs and SANs on the back end to increase performance and robustness, and by caching on the client end to reduce load, but it is still limited. Ultimately, all decisions are moderated by a single host which must communicate directly with all clients, so at larger scale the network bandwidth becomes a problem as well.

At the other extreme, coordinations is pushed completely to the clients. This is essentially the centralized model turned upside down. Instead of many clients talking to a single server, a single client must communicate with many servers. Instead of one congestion point, every client becomes overwhelmed with bookkeeping. In practice, some structure can be introduced so that the system isn't a full $n$-way clique---the host for each object of storage may become the mediator for access to that object, for example---but making every participant a symmetric peer is mostly useful for privacy or other non-technical concerns.

A more practical approach is to emulate a centralized architecture as much as possible, since this provides the simplest model with the least administrative overhead, and carve up the problem enough to achieve the desired scaling properties. The two extremes offer the greatest conceptual purity, but the practical benefits are to be had somewhere between them. Most P2P services now introduce some kind of a hierarchy that designates some hosts as ``superpeers'' or ``supernodes'' that act as aggregation points for a manageable set of participants. These distinguished hosts only need to communicate directly with other similarly endowed hosts, potentially reducing the visible size of the network by orders of magnitude. The world is small enough and computers big enough that a few orders of magnitude is enough reduction to make many distributed problems tractable.

Another way of viewing this problem is to consider how many manually administered systems are often implemented today. NFS \cite{callaghan} is still one of the most widely-used systems, and a workstation environment may include a series of NFS servers to provide storage for a larger number of client workstations. Each server will be backed by a RAID \cite{patterson} system for improved performance and reliability, and the administrators may divide the file system namespace in order to balance average demand over the different servers. While most file access in uncontended, having a single server that serves all interested clients makes coherence easy (ignoring the effects of caching).

An interesting thought exercise is to consider the same set of workstations locking into a long-term steady-state behavior, and then asking the systems administrator to hand configure the system based on those usage patterns. One reasonable possibility would be to locate files or branches of the namespace tree used exclusively by a single workstation on that workstation. When two or more users require the same file or set of files, it could be managed by the workstation that uses it the most and exported via NFS (or a similar client-server file system protocol) to the other participants. Since contention is relatively rare, this would reduce network traffic for the storage system to a low level for as long as the hypothetical steady-state continues. One of the goals of the Envoy design is to capture this approximate topology.

In service clusters, physical hosts are divided into roughly 10s of services, which may be transient, untrusted, and unreliable. This increases the number of participants requiring access to the storage system by the same factor and exposes difficult security problems. One of the principle benefits of isolating services in individual virtual machines is in reducing the impact of services that fail, either benignly or maliciously, and trusting the administration of the file system to the services negates this benefit.

The environment includes a trusted management domain on each server, however, which introduces a natural aggregation point and a way to sidestep many of those problems.

\subsection{Local impact}

Envoy is designed according to the principle of \textit{local impact}, meaning that the resources consumed directly or indirectly by a service should be as close to that service as possible. If not in the same VM, then on the same machine, or on another machine that has some specific reason to yield its resources to a remote service.

By extension, this principle leads to a topology that is shaped according to runtime conflicts. When there is no reason to suspect contention, machines will prefer to assume complete control over the storage in active use by their client services. If two machines must explicitly coordinate their access to storage, they are treading on overlapping or neighboring storage and implicitly declaring that a conflict is likely to occur.

Previous studies of file system traffic have repeatedly concluded that runtime contention is rare, so Envoy is designed to assume that exclusive access dominates and react to conflicts as they occur rather than optimizing for the occasions when access overlaps.

\subsection{Supporting services}

VM migration \cite{clark} is essential for load balancing and uninterrupted service in the presence of hardware servicing.

To support migration, a file system cannot be tied to a single physical machine. Any given file system image must be as mobile as the service that relies on it.

One of the attractive features of running individual services in virtual machines is the flexibility of management this model offers. Services can be decoupled, instances can be created and destroyed easily in response to need, and a new service can be deployed without dedicating a new machine to it, removing and old service to make room, or studying the potential interactions that would occur if a shared server was the only option. To this end, lightweight operations are required to instantiate and clone services.

The unit of management is an entire operating system on a virtual machine. Installing an operating system for each service instance would be prohibitively slow, labour intensive, and wasteful of space. A typical installation of Linux using Fedora Core~3 takes about 50 terabytes of space, and an automated installation is unlikely to give the right set of installed packages and running daemons, so some customization will be necessary.

Instead of trying to automate installations, Envoy offers lightweight operations to fork a file system.

Snapshots allow simple backup and recovery, and time travel for debugging and forensics \cite{whitaker}.

\section{Summary}
